{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m torch\u001b[39m.\u001b[39mset_default_device(device)\n\u001b[0;32m     86\u001b[0m \u001b[39m# Load model and tokenizer\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m     88\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mqnguyen3/nanoLLaVA\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     89\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16,\n\u001b[0;32m     90\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     91\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     92\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     93\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mqnguyen3/nanoLLaVA\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     94\u001b[0m     trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     96\u001b[0m \u001b[39m# Process images\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:556\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    555\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mregister(config\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, model_class, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 556\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m    557\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    558\u001b[0m     )\n\u001b[0;32m    559\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    560\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\transformers\\modeling_utils.py:3164\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3148\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   3149\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   3150\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m   3151\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcache_dir\u001b[39m\u001b[39m\"\u001b[39m: cache_dir,\n\u001b[0;32m   3152\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mforce_download\u001b[39m\u001b[39m\"\u001b[39m: force_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3162\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m: commit_hash,\n\u001b[0;32m   3163\u001b[0m     }\n\u001b[1;32m-> 3164\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_file_kwargs)\n\u001b[0;32m   3166\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   3167\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   3168\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[0;32m   3169\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\transformers\\utils\\hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    396\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    399\u001b[0m         path_or_repo_id,\n\u001b[0;32m    400\u001b[0m         filename,\n\u001b[0;32m    401\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    402\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    403\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    404\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    405\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    406\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    407\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    408\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    409\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m    410\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    411\u001b[0m     )\n\u001b[0;32m    412\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    413\u001b[0m     resolved_file \u001b[39m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\huggingface_hub\\file_download.py:1492\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1489\u001b[0m         \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1490\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[1;32m-> 1492\u001b[0m     http_get(\n\u001b[0;32m   1493\u001b[0m         url_to_download,\n\u001b[0;32m   1494\u001b[0m         temp_file,\n\u001b[0;32m   1495\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1496\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1497\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1498\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[0;32m   1499\u001b[0m         displayed_filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[0;32m   1500\u001b[0m     )\n\u001b[0;32m   1502\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1503\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\huggingface_hub\\file_download.py:547\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries)\u001b[0m\n\u001b[0;32m    545\u001b[0m new_resume_size \u001b[39m=\u001b[39m resume_size\n\u001b[0;32m    546\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 547\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    548\u001b[0m         \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    549\u001b[0m             progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\http\\client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    457\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 459\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    463\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\http\\client.py:503\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    498\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    500\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 503\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    505\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    506\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\ssl.py:1132\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1133\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "# Define paths\n",
    "DATASET_PATH = r\"C:\\Users\\Patrick\\Documents\\thesis\\Dataset\\OwnDataSet\"\n",
    "RESULTS_PATH = r\"C:\\Users\\Patrick\\Documents\\thesis\\Dataset\\Results\"\n",
    "\n",
    "# Disable some warnings\n",
    "transformers.logging.set_verbosity_error()\n",
    "transformers.logging.disable_progress_bar()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_image(image_file):\n",
    "    return Image.open(image_file).convert('RGB')\n",
    "\n",
    "def process_images(model, tokenizer, dataset_path):\n",
    "    results = []\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            img_path = os.path.join(dataset_path, filename)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Load image\n",
    "            image = load_image(img_path)\n",
    "            \n",
    "            # Generate description\n",
    "            prompt = 'Please describe the image shortly Maximum 150 characters.'\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": f'<image>\\n{prompt}'}\n",
    "            ]\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            text_chunks = [tokenizer(chunk).input_ids for chunk in text.split('<image>')]\n",
    "            input_ids = torch.tensor(text_chunks[0] + [-200] + text_chunks[1], dtype=torch.long).unsqueeze(0)\n",
    "            \n",
    "            image_tensor = model.process_images([image], model.config).to(dtype=model.dtype)\n",
    "            \n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                images=image_tensor,\n",
    "                max_new_tokens=150,\n",
    "                use_cache=True)[0]\n",
    "            \n",
    "            response = tokenizer.decode(output_ids[input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            processing_time = end_time - start_time\n",
    "            output_tokens = len(tokenizer.encode(response))\n",
    "            \n",
    "            results.append({\n",
    "                \"filename\": filename,\n",
    "                \"processing_time\": processing_time,\n",
    "                \"output_tokens\": output_tokens,\n",
    "                \"alternative_text\": response\n",
    "            })\n",
    "            \n",
    "            print(f\"Processed {filename}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_results(results, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    output_file = os.path.join(output_path, \"nanoLLaVA_analysis_results.json\")\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Setup for CPU or GPU\n",
    "use_cpu = True  # Set this to False if you want to use GPU\n",
    "device = 'cpu' if use_cpu else 'cuda'\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'qnguyen3/nanoLLaVA',\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'qnguyen3/nanoLLaVA',\n",
    "    trust_remote_code=True)\n",
    "\n",
    "# Process images\n",
    "results = process_images(model, tokenizer, DATASET_PATH)\n",
    "\n",
    "# Print summary\n",
    "total_time = sum(r[\"processing_time\"] for r in results)\n",
    "total_output_tokens = sum(r[\"output_tokens\"] for r in results)\n",
    "num_images = len(results)\n",
    "\n",
    "print(f\"Processed {num_images} images\")\n",
    "print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "print(f\"Average time per image: {total_time/num_images:.2f} seconds\")\n",
    "print(f\"Total output tokens: {total_output_tokens}\")\n",
    "print(f\"Average output tokens per image: {total_output_tokens/num_images:.2f}\")\n",
    "\n",
    "# Save results\n",
    "save_results(results, RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the process...\n",
      "Loading processor...\n",
      "Processor loaded.\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Downloading image...\n",
      "Image downloaded and converted.\n",
      "Processing inputs...\n",
      "Inputs processed.\n",
      "Generating output...\n",
      "Output generated.\n",
      "Result: \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "print(\"Starting the process...\")\n",
    "\n",
    "print(\"Loading processor...\")\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "print(\"Processor loaded.\")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "print(\"Downloading image...\")\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "print(\"Image downloaded and converted.\")\n",
    "\n",
    "question = \"What is on the image?\"\n",
    "print(\"Processing inputs...\")\n",
    "inputs = processor(raw_image, question, return_tensors=\"pt\")\n",
    "print(\"Inputs processed.\")\n",
    "\n",
    "print(\"Generating output...\")\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_new_tokens=50)\n",
    "print(\"Output generated.\")\n",
    "\n",
    "result = processor.decode(out[0], skip_special_tokens=True).strip()\n",
    "print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Blip2Processor:\n",
       "- image_processor: BlipImageProcessor {\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"BlipImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"processor_class\": \"Blip2Processor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}\n",
       "\n",
       "- tokenizer: GPT2TokenizerFast(name_or_path='Salesforce/blip2-opt-2.7b', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       "\n",
       "{\n",
       "  \"processor_class\": \"Blip2Processor\"\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating output...\n",
      "Output generated.\n",
      "Raw output: tensor([[50118]])\n",
      "Output shape: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating output...\")\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_new_tokens=50)\n",
    "print(\"Output generated.\")\n",
    "print(\"Raw output:\", out)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "Manual decoding result: Ċ\n"
     ]
    }
   ],
   "source": [
    "result = processor.decode(out[0], skip_special_tokens=True).strip()\n",
    "print(\"Result:\", result)\n",
    "\n",
    "# Manual decoding\n",
    "vocab = processor.tokenizer.get_vocab()\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "manual_result = ' '.join([inv_vocab.get(token.item(), '[UNK]') for token in out[0]])\n",
    "print(\"Manual decoding result:\", manual_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([1, 7])\n",
      "Attention mask shape: torch.Size([1, 7])\n",
      "Sample of input IDs: tensor([   2, 2264,   16,   15,    5, 2274,  116])\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs shape:\", inputs['input_ids'].shape)\n",
    "print(\"Attention mask shape:\", inputs['attention_mask'].shape)\n",
    "print(\"Sample of input IDs:\", inputs['input_ids'][0][:10])  # Print first 10 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray, but got <class 'str'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Simple generation test\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_input \u001b[39m=\u001b[39m processor(\u001b[39m\"\u001b[39;49m\u001b[39mWhat is the capital of France?\u001b[39;49m\u001b[39m\"\u001b[39;49m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m test_output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtest_input, max_new_tokens\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[0;32m      4\u001b[0m test_result \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mdecode(test_output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\transformers\\models\\blip_2\\processing_blip_2.py:105\u001b[0m, in \u001b[0;36mBlip2Processor.__call__\u001b[1;34m(self, images, text, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_token_type_ids, return_length, verbose, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m text_encoding\n\u001b[0;32m    104\u001b[0m \u001b[39m# add pixel_values\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m encoding_image_processor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_processor(images, return_tensors\u001b[39m=\u001b[39;49mreturn_tensors)\n\u001b[0;32m    107\u001b[0m \u001b[39mif\u001b[39;00m text \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     text_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[0;32m    109\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m    110\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    125\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\transformers\\image_processing_utils.py:551\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[1;34m(self, images, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[0;32m    550\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(images, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\transformers\\models\\blip\\image_processing_blip.py:235\u001b[0m, in \u001b[0;36mBlipImageProcessor.preprocess\u001b[1;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, do_convert_rgb, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m size \u001b[39m=\u001b[39m size \u001b[39mif\u001b[39;00m size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize\n\u001b[0;32m    233\u001b[0m size \u001b[39m=\u001b[39m get_size_dict(size, default_to_square\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 235\u001b[0m images \u001b[39m=\u001b[39m make_list_of_images(images)\n\u001b[0;32m    237\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid_images(images):\n\u001b[0;32m    238\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    239\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    241\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\transformers\\image_utils.py:162\u001b[0m, in \u001b[0;36mmake_list_of_images\u001b[1;34m(images, expected_ndims)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    158\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid image shape. Expected either \u001b[39m\u001b[39m{\u001b[39;00mexpected_ndims\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m or \u001b[39m\u001b[39m{\u001b[39;00mexpected_ndims\u001b[39m}\u001b[39;00m\u001b[39m dimensions, but got\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mimages\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m dimensions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         )\n\u001b[0;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m images\n\u001b[1;32m--> 162\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    163\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mInvalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mjax.ndarray, but got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(images)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    165\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray, but got <class 'str'>."
     ]
    }
   ],
   "source": [
    "# Simple generation test\n",
    "test_input = processor(\"What is the capital of France?\", return_tensors=\"pt\")\n",
    "test_output = model.generate(**test_input, max_new_tokens=50)\n",
    "test_result = processor.decode(test_output[0], skip_special_tokens=True).strip()\n",
    "print(\"Test result:\", test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: a woman sitting on the beach with her dog\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "    pixel_values=inputs['pixel_values'],\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_new_tokens=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and processor loaded.\n",
      "Processing images...\n",
      "Processed 01.jpg\n",
      "Processed 02.jpg\n",
      "Processed 03.jpg\n",
      "Processed 04.jpg\n",
      "Processed 05.jpg\n",
      "Processed 06.png\n",
      "Processed 07.png\n",
      "Processed 08.png\n",
      "Processed 09.png\n",
      "Processed 10.png\n",
      "Processed 11.png\n",
      "Processed 12.png\n",
      "Processed 13.png\n",
      "Processed 14.jpg\n",
      "Processed 15.png\n",
      "Processed 15 images\n",
      "Total processing time: 273.73 seconds\n",
      "Average time per image: 18.25 seconds\n",
      "Total output tokens: 15\n",
      "Average output tokens per image: 1.00\n",
      "Results saved to C:\\Users\\Patrick\\Documents\\thesis\\Dataset\\Results\\blip2_analysis_results.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "# Define paths\n",
    "DATASET_PATH = r\"C:\\Users\\Patrick\\Documents\\thesis\\Dataset\\OwnDataSet\"\n",
    "RESULTS_PATH = r\"C:\\Users\\Patrick\\Documents\\thesis\\Dataset\\Results\"\n",
    "\n",
    "# Disable some warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_image(image_file):\n",
    "    return Image.open(image_file).convert('RGB')\n",
    "\n",
    "def process_images(model, processor, dataset_path):\n",
    "    results = []\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            img_path = os.path.join(dataset_path, filename)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Load and process image\n",
    "            image = load_image(img_path)\n",
    "            \n",
    "            # Generate description\n",
    "            prompt = 'Please describe the image shortly Maximum 150 characters.'\n",
    "            inputs = processor(image, prompt, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=150,\n",
    "                    num_beams=5,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            \n",
    "            response = processor.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            processing_time = end_time - start_time\n",
    "            output_tokens = len(processor.tokenizer.encode(response))\n",
    "            \n",
    "            results.append({\n",
    "                \"filename\": filename,\n",
    "                \"processing_time\": processing_time,\n",
    "                \"output_tokens\": output_tokens,\n",
    "                \"alternative_text\": response\n",
    "            })\n",
    "            \n",
    "            print(f\"Processed {filename}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_results(results, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    output_file = os.path.join(output_path, \"blip2_analysis_results.json\")\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Setup for CPU or GPU\n",
    "use_cpu = True  # Set this to False if you want to use GPU\n",
    "\n",
    "# Load model and processor\n",
    "print(\"Loading model and processor...\")\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "if use_cpu:\n",
    "    model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "        \"Salesforce/blip2-opt-2.7b\",\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map=\"cpu\"\n",
    "    )\n",
    "else:\n",
    "    model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "        \"Salesforce/blip2-opt-2.7b\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "print(\"Model and processor loaded.\")\n",
    "\n",
    "# Process images\n",
    "print(\"Processing images...\")\n",
    "results = process_images(model, processor, DATASET_PATH)\n",
    "\n",
    "# Print summary\n",
    "total_time = sum(r[\"processing_time\"] for r in results)\n",
    "total_output_tokens = sum(r[\"output_tokens\"] for r in results)\n",
    "num_images = len(results)\n",
    "print(f\"Processed {num_images} images\")\n",
    "print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "print(f\"Average time per image: {total_time/num_images:.2f} seconds\")\n",
    "print(f\"Total output tokens: {total_output_tokens}\")\n",
    "print(f\"Average output tokens per image: {total_output_tokens/num_images:.2f}\")\n",
    "\n",
    "# Save results\n",
    "save_results(results, RESULTS_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
