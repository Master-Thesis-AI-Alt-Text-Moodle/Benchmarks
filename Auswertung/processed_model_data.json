[
  {
    "Model": "Qwen-VL",
    "Source": "https://github.com/QwenLM/Qwen-VL",
    "Parameters": "9.6B",
    "Time": "2023/8/24",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 52.5,
    "CCBench Overall": 6.1,
    "MMBench_TEST_EN Overall": 32.2,
    "MMBench_TEST_CN Overall": 7.8,
    "MMBench_TEST_EN_V11 Overall": 41.6,
    "MMBench_TEST_CN_V11 Overall": 24.2,
    "MME Overall": 482.7,
    "MMVet Overall": 13.0,
    "MMMU_VAL Overall": 29.6,
    "MathVista Overall": 15.5,
    "HallusionBench Overall": 29.9,
    "LLaVABench Overall": 12.9,
    "AI2D Overall": 57.7,
    "ScienceQA_VAL Overall": 57.7,
    "ScienceQA_TEST Overall": 61.1,
    "MMStar Overall": 32.5,
    "RealWorldQA Overall": 37.8,
    "TextVQA_VAL Overall": 63.1,
    "ChartQA_TEST Overall": 59.0,
    "OCRVQA_TESTCORE Overall": 67.5,
    "POPE Overall": 70.0,
    "SEEDBench2_Plus Overall": 40.1,
    "MMT-Bench_VAL Overall": 42.9,
    "BLINK Overall": 27.9
  },
  {
    "Model": "Qwen-VL-Chat",
    "Source": "https://github.com/QwenLM/Qwen-VL",
    "Parameters": "9.6B",
    "Time": "2023/8/24",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 64.8,
    "CCBench Overall": 41.2,
    "MMBench_TEST_EN Overall": 61.8,
    "MMBench_TEST_CN Overall": 56.3,
    "MMBench_TEST_EN_V11 Overall": 60.8,
    "MMBench_TEST_CN_V11 Overall": 57.5,
    "MME Overall": 1860.0,
    "MMVet Overall": 47.3,
    "MMMU_VAL Overall": 37.0,
    "MathVista Overall": 34.9,
    "HallusionBench Overall": 36.8,
    "LLaVABench Overall": 67.7,
    "AI2D Overall": 63.0,
    "ScienceQA_VAL Overall": 65.5,
    "ScienceQA_TEST Overall": 68.8,
    "MMStar Overall": 34.5,
    "RealWorldQA Overall": 49.3,
    "TextVQA_VAL Overall": 60.7,
    "ChartQA_TEST Overall": 49.8,
    "OCRVQA_TESTCORE Overall": 58.6,
    "POPE Overall": 74.9,
    "SEEDBench2_Plus Overall": 46.0,
    "MMT-Bench_VAL Overall": 47.6,
    "BLINK Overall": 28.2
  },
  {
    "Model": "PandaGPT-13B",
    "Source": "https://arxiv.org/abs/2305.16355",
    "Parameters": "14B",
    "Time": "2023/5/25",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 47.6,
    "CCBench Overall": 6.7,
    "MMBench_TEST_EN Overall": 42.5,
    "MMBench_TEST_CN Overall": 32.0,
    "MMBench_TEST_EN_V11 Overall": 39.7,
    "MMBench_TEST_CN_V11 Overall": 29.2,
    "MME Overall": 1072.2,
    "MMVet Overall": 19.6,
    "MMMU_VAL Overall": 32.9,
    "MathVista Overall": 25.0,
    "HallusionBench Overall": 20.0,
    "LLaVABench Overall": 37.1,
    "AI2D Overall": 50.3,
    "ScienceQA_VAL Overall": 60.9,
    "ScienceQA_TEST Overall": 63.2,
    "MMStar Overall": 25.6,
    "RealWorldQA Overall": 32.8,
    "POPE Overall": 75.1,
    "SEEDBench2_Plus Overall": 29.6,
    "MMT-Bench_VAL Overall": 32.4
  },
  {
    "Model": "OpenFlamingo v2",
    "Source": "https://github.com/mlfoundations/open_flamingo",
    "Parameters": "9B",
    "Time": "2023/6/28",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 28.8,
    "CCBench Overall": 6.3,
    "MMBench_TEST_EN Overall": 5.7,
    "MMBench_TEST_CN Overall": 14.4,
    "MMBench_TEST_EN_V11 Overall": 2.4,
    "MMBench_TEST_CN_V11 Overall": 13.3,
    "MME Overall": 607.2,
    "MMVet Overall": 23.3,
    "MMMU_VAL Overall": 28.8,
    "MathVista Overall": 18.6,
    "HallusionBench Overall": 29.4,
    "LLaVABench Overall": 34.2,
    "AI2D Overall": 31.7,
    "ScienceQA_VAL Overall": 45.7,
    "ScienceQA_TEST Overall": 44.8,
    "MMStar Overall": 26.9,
    "RealWorldQA Overall": 35.2,
    "TextVQA_VAL Overall": 16.3,
    "POPE Overall": 52.6,
    "SEEDBench2_Plus Overall": 28.7,
    "MMT-Bench_VAL Overall": 28.6
  },
  {
    "Model": "ShareGPT4V-7B",
    "Source": "https://sharegpt4v.github.io",
    "Parameters": "7.2B",
    "Time": "2023/12/08",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 69.3,
    "CCBench Overall": 30.8,
    "MMBench_TEST_EN Overall": 67.6,
    "MMBench_TEST_CN Overall": 60.7,
    "MMBench_TEST_EN_V11 Overall": 64.6,
    "MMBench_TEST_CN_V11 Overall": 58.6,
    "MME Overall": 1914.9,
    "MMVet Overall": 33.4,
    "MMMU_VAL Overall": 37.2,
    "MathVista Overall": 26.5,
    "HallusionBench Overall": 28.6,
    "LLaVABench Overall": 66.9,
    "AI2D Overall": 58.0,
    "ScienceQA_VAL Overall": 68.2,
    "ScienceQA_TEST Overall": 69.5,
    "MMStar Overall": 35.7,
    "RealWorldQA Overall": 54.9,
    "TextVQA_VAL Overall": 51.1,
    "ChartQA_TEST Overall": 21.3,
    "OCRVQA_TESTCORE Overall": 63.4,
    "POPE Overall": 86.6,
    "SEEDBench2_Plus Overall": 46.1,
    "MMT-Bench_VAL Overall": 51.2,
    "BLINK Overall": 40.9
  },
  {
    "Model": "ShareGPT4V-13B",
    "Source": "https://sharegpt4v.github.io",
    "Parameters": "13.4B",
    "Time": "2023/12/08",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 70.6,
    "CCBench Overall": 30.4,
    "MMBench_TEST_EN Overall": 69.8,
    "MMBench_TEST_CN Overall": 65.1,
    "MMBench_TEST_EN_V11 Overall": 67.5,
    "MMBench_TEST_CN_V11 Overall": 63.0,
    "MME Overall": 1853.1,
    "MMVet Overall": 41.1,
    "MMMU_VAL Overall": 36.6,
    "MathVista Overall": 29.3,
    "HallusionBench Overall": 28.4,
    "LLaVABench Overall": 69.1,
    "AI2D Overall": 61.4,
    "ScienceQA_VAL Overall": 70.7,
    "ScienceQA_TEST Overall": 72.6,
    "MMStar Overall": 38.3,
    "RealWorldQA Overall": 57.0,
    "TextVQA_VAL Overall": 52.7,
    "ChartQA_TEST Overall": 24.6,
    "OCRVQA_TESTCORE Overall": 64.3,
    "POPE Overall": 87.5,
    "SEEDBench2_Plus Overall": 47.7,
    "MMT-Bench_VAL Overall": 52.0,
    "BLINK Overall": 41.6
  },
  {
    "Model": "TransCore-M",
    "Source": "https://github.com/PCIResearch/TransCore-M",
    "Parameters": "13.4B",
    "Time": "2024/04/16",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 72.0,
    "CCBench Overall": 69.8,
    "MMBench_TEST_EN Overall": 82.3,
    "MMBench_TEST_CN Overall": 80.7,
    "MMBench_TEST_EN_V11 Overall": 67.7,
    "MMBench_TEST_CN_V11 Overall": 67.9,
    "MME Overall": 1885.0,
    "MMVet Overall": 36.9,
    "MMMU_VAL Overall": 41.0,
    "MathVista Overall": 32.3,
    "HallusionBench Overall": 27.3,
    "LLaVABench Overall": 66.8,
    "AI2D Overall": 64.1,
    "ScienceQA_VAL Overall": 74.4,
    "ScienceQA_TEST Overall": 74.9,
    "MMStar Overall": 35.6,
    "RealWorldQA Overall": 57.1,
    "TextVQA_VAL Overall": 55.2,
    "ChartQA_TEST Overall": 28.0,
    "OCRVQA_TESTCORE Overall": 60.9,
    "POPE Overall": 87.1,
    "SEEDBench2_Plus Overall": 51.1,
    "MMT-Bench_VAL Overall": 54.1,
    "BLINK Overall": 42.7
  },
  {
    "Model": "InstructBLIP-7B",
    "Source": "https://arxiv.org/abs/2305.06500",
    "Parameters": "8B",
    "Time": "2023/5/11",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 44.5,
    "CCBench Overall": 12.7,
    "MMBench_TEST_EN Overall": 33.9,
    "MMBench_TEST_CN Overall": 23.9,
    "MMBench_TEST_EN_V11 Overall": 38.4,
    "MMBench_TEST_CN_V11 Overall": 18.4,
    "MME Overall": 1391.4,
    "MMVet Overall": 33.1,
    "MMMU_VAL Overall": 30.6,
    "MathVista Overall": 24.4,
    "HallusionBench Overall": 31.2,
    "LLaVABench Overall": 59.8,
    "AI2D Overall": 40.6,
    "ScienceQA_VAL Overall": 54.7,
    "ScienceQA_TEST Overall": 54.1,
    "MMStar Overall": 32.7,
    "RealWorldQA Overall": 36.9,
    "TextVQA_VAL Overall": 33.6,
    "ChartQA_TEST Overall": 10.9,
    "OCRVQA_TESTCORE Overall": 50.2,
    "POPE Overall": 86.1,
    "SEEDBench2_Plus Overall": 29.5,
    "MMT-Bench_VAL Overall": 35.1
  },
  {
    "Model": "VisualGLM",
    "Source": "https://github.com/THUDM/VisualGLM-6B",
    "Parameters": "8B",
    "Time": "2023/5/17",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 47.0,
    "CCBench Overall": 17.3,
    "MMBench_TEST_EN Overall": 37.6,
    "MMBench_TEST_CN Overall": 35.5,
    "MMBench_TEST_EN_V11 Overall": 36.7,
    "MMBench_TEST_CN_V11 Overall": 34.6,
    "MME Overall": 738.1,
    "MMVet Overall": 14.8,
    "MMMU_VAL Overall": 29.9,
    "MathVista Overall": 21.9,
    "HallusionBench Overall": 25.0,
    "LLaVABench Overall": 37.3,
    "AI2D Overall": 41.2,
    "ScienceQA_VAL Overall": 53.4,
    "ScienceQA_TEST Overall": 56.1,
    "MMStar Overall": 25.9,
    "RealWorldQA Overall": 39.9,
    "POPE Overall": 70.4,
    "SEEDBench2_Plus Overall": 30.2,
    "MMT-Bench_VAL Overall": 30.6
  },
  {
    "Model": "ShareCaptioner",
    "Source": "https://huggingface.co/Lin-Chen/ShareCaptioner/",
    "Parameters": "8B",
    "Time": "2024/01/03",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 61.2,
    "CCBench Overall": 40.4,
    "MMBench_TEST_EN Overall": 66.5,
    "MMBench_TEST_CN Overall": 60.9,
    "MMBench_TEST_EN_V11 Overall": 63.9,
    "MMBench_TEST_CN_V11 Overall": 57.8,
    "MME Overall": 1642.5,
    "MMVet Overall": 30.1,
    "MMMU_VAL Overall": 36.3,
    "MathVista Overall": 29.1,
    "HallusionBench Overall": 34.2,
    "LLaVABench Overall": 47.4,
    "AI2D Overall": 56.7,
    "ScienceQA_VAL Overall": 81.0,
    "ScienceQA_TEST Overall": 82.3,
    "MMStar Overall": 38.4,
    "RealWorldQA Overall": 52.7,
    "TextVQA_VAL Overall": 38.0,
    "OCRVQA_TESTCORE Overall": 50.2,
    "POPE Overall": 84.4,
    "SEEDBench2_Plus Overall": 42.4,
    "MMT-Bench_VAL Overall": 47.6
  },
  {
    "Model": "Monkey",
    "Source": "https://github.com/Yuliang-Liu/Monkey",
    "Parameters": "9.8B",
    "Time": "2024/01/11",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 64.3,
    "CCBench Overall": 37.1,
    "MMBench_TEST_EN Overall": 59.6,
    "MMBench_TEST_CN Overall": 54.7,
    "MMBench_TEST_EN_V11 Overall": 57.4,
    "MMBench_TEST_CN_V11 Overall": 52.5,
    "MME Overall": 1759.9,
    "MMVet Overall": 38.1,
    "MMMU_VAL Overall": 38.9,
    "MathVista Overall": 33.5,
    "HallusionBench Overall": 34.9,
    "LLaVABench Overall": 43.3,
    "AI2D Overall": 62.5,
    "ScienceQA_VAL Overall": 68.2,
    "ScienceQA_TEST Overall": 72.1,
    "MMStar Overall": 37.0,
    "RealWorldQA Overall": 51.6,
    "TextVQA_VAL Overall": 65.8,
    "ChartQA_TEST Overall": 59.0,
    "OCRVQA_TESTCORE Overall": 67.0,
    "POPE Overall": 83.7,
    "SEEDBench2_Plus Overall": 52.0,
    "MMT-Bench_VAL Overall": 49.0,
    "BLINK Overall": 37.7
  },
  {
    "Model": "Monkey-Chat",
    "Source": "https://github.com/Yuliang-Liu/Monkey",
    "Parameters": "9.8B",
    "Time": "2024/01/16",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 68.9,
    "CCBench Overall": 48.0,
    "MMBench_TEST_EN Overall": 72.4,
    "MMBench_TEST_CN Overall": 67.5,
    "MMBench_TEST_EN_V11 Overall": 69.6,
    "MMBench_TEST_CN_V11 Overall": 65.1,
    "MME Overall": 1887.4,
    "MMVet Overall": 41.0,
    "MMMU_VAL Overall": 40.7,
    "MathVista Overall": 35.9,
    "HallusionBench Overall": 39.3,
    "LLaVABench Overall": 60.5,
    "AI2D Overall": 68.5,
    "ScienceQA_VAL Overall": 80.8,
    "ScienceQA_TEST Overall": 82.4,
    "MMStar Overall": 40.7,
    "RealWorldQA Overall": 52.4,
    "TextVQA_VAL Overall": 65.5,
    "ChartQA_TEST Overall": 59.5,
    "OCRVQA_TESTCORE Overall": 64.4,
    "POPE Overall": 83.5,
    "SEEDBench2_Plus Overall": 54.9,
    "MMT-Bench_VAL Overall": 53.3,
    "BLINK Overall": 41.2
  },
  {
    "Model": "MiniGPT-4-v2",
    "Source": "https://minigpt-v2.github.io",
    "Parameters": "8B",
    "Time": "2023/10/27",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 29.4,
    "CCBench Overall": 1.4,
    "MMBench_TEST_EN Overall": 9.4,
    "MMBench_TEST_CN Overall": 4.7,
    "MMBench_TEST_EN_V11 Overall": 5.0,
    "MMBench_TEST_CN_V11 Overall": 1.4,
    "MME Overall": 968.4,
    "MMVet Overall": 10.5,
    "MMMU_VAL Overall": 25.0,
    "MathVista Overall": 23.1,
    "HallusionBench Overall": 30.0,
    "LLaVABench Overall": 28.8,
    "AI2D Overall": 30.5,
    "ScienceQA_VAL Overall": 54.1,
    "ScienceQA_TEST Overall": 54.7,
    "MMStar Overall": 21.3,
    "RealWorldQA Overall": 30.7,
    "POPE Overall": 60.0,
    "SEEDBench2_Plus Overall": 23.3,
    "MMT-Bench_VAL Overall": 22.1
  },
  {
    "Model": "MiniGPT-4-v1-7B",
    "Source": "https://arxiv.org/abs/2304.10592",
    "Parameters": "8B",
    "Time": "2023/4/20",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 31.6,
    "CCBench Overall": 1.8,
    "MMBench_TEST_EN Overall": 23.0,
    "MMBench_TEST_CN Overall": 11.9,
    "MMBench_TEST_EN_V11 Overall": 30.9,
    "MMBench_TEST_CN_V11 Overall": 10.7,
    "MME Overall": 1047.4,
    "MMVet Overall": 15.6,
    "MMMU_VAL Overall": 23.6,
    "MathVista Overall": 20.4,
    "HallusionBench Overall": 31.9,
    "LLaVABench Overall": 45.1,
    "AI2D Overall": 28.4,
    "ScienceQA_VAL Overall": 39.0,
    "ScienceQA_TEST Overall": 39.6,
    "MMStar Overall": 16.3,
    "RealWorldQA Overall": 21.3,
    "POPE Overall": 34.6,
    "SEEDBench2_Plus Overall": 15.2,
    "MMT-Bench_VAL Overall": 16.5
  },
  {
    "Model": "Emu2_chat",
    "Source": "https://github.com/baaivision/Emu/tree/main/Emu2",
    "Parameters": "37B",
    "Time": "2024/01/17",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 68.9,
    "CCBench Overall": 18.8,
    "MMBench_TEST_EN Overall": 63.6,
    "MMBench_TEST_CN Overall": 45.9,
    "MMBench_TEST_EN_V11 Overall": 61.3,
    "MMBench_TEST_CN_V11 Overall": 44.3,
    "MME Overall": 1678.0,
    "MMVet Overall": 31.0,
    "MMMU_VAL Overall": 35.0,
    "MathVista Overall": 30.7,
    "HallusionBench Overall": 29.5,
    "LLaVABench Overall": 56.4,
    "AI2D Overall": 49.7,
    "ScienceQA_VAL Overall": 65.3,
    "ScienceQA_TEST Overall": 68.2,
    "MMStar Overall": 40.7,
    "RealWorldQA Overall": 57.3,
    "TextVQA_VAL Overall": 67.6,
    "ChartQA_TEST Overall": 26.8,
    "OCRVQA_TESTCORE Overall": 41.0,
    "POPE Overall": 88.0,
    "SEEDBench2_Plus Overall": 42.1
  },
  {
    "Model": "Yi-VL-6B",
    "Source": "https://huggingface.co/01-ai/Yi-VL-6B",
    "Parameters": "6.6B",
    "Time": "2024/01/25",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 67.5,
    "CCBench Overall": 42.9,
    "MMBench_TEST_EN Overall": 68.4,
    "MMBench_TEST_CN Overall": 66.6,
    "MMBench_TEST_EN_V11 Overall": 65.2,
    "MMBench_TEST_CN_V11 Overall": 63.1,
    "MME Overall": 1915.1,
    "MMVet Overall": 32.1,
    "MMMU_VAL Overall": 40.3,
    "MathVista Overall": 29.7,
    "HallusionBench Overall": 36.0,
    "LLaVABench Overall": 51.9,
    "AI2D Overall": 59.8,
    "ScienceQA_VAL Overall": 71.7,
    "ScienceQA_TEST Overall": 72.6,
    "MMStar Overall": 37.7,
    "RealWorldQA Overall": 53.5,
    "TextVQA_VAL Overall": 44.8,
    "ChartQA_TEST Overall": 15.5,
    "OCRVQA_TESTCORE Overall": 49.5,
    "POPE Overall": 82.5,
    "SEEDBench2_Plus Overall": 46.2,
    "MMT-Bench_VAL Overall": 52.8
  },
  {
    "Model": "Yi-VL-34B",
    "Source": "https://huggingface.co/01-ai/Yi-VL-34B",
    "Parameters": "34.6B",
    "Time": "2024/01/25",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 68.1,
    "CCBench Overall": 46.1,
    "MMBench_TEST_EN Overall": 72.4,
    "MMBench_TEST_CN Overall": 70.7,
    "MMBench_TEST_EN_V11 Overall": 68.7,
    "MMBench_TEST_CN_V11 Overall": 66.8,
    "MME Overall": 2050.2,
    "MMVet Overall": 32.7,
    "MMMU_VAL Overall": 45.1,
    "MathVista Overall": 31.5,
    "HallusionBench Overall": 35.3,
    "LLaVABench Overall": 62.3,
    "AI2D Overall": 65.9,
    "ScienceQA_VAL Overall": 73.6,
    "ScienceQA_TEST Overall": 75.5,
    "MMStar Overall": 40.5,
    "RealWorldQA Overall": 54.8,
    "TextVQA_VAL Overall": 42.5,
    "ChartQA_TEST Overall": 14.0,
    "OCRVQA_TESTCORE Overall": 42.4,
    "POPE Overall": 82.5,
    "SEEDBench2_Plus Overall": 47.5,
    "MMT-Bench_VAL Overall": 53.4
  },
  {
    "Model": "OmniLMM-12B",
    "Source": "https://huggingface.co/openbmb/OmniLMM-12B",
    "Parameters": "12B",
    "Time": "2024/02/07",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 71.5,
    "CCBench Overall": 37.1,
    "MMBench_TEST_EN Overall": 71.7,
    "MMBench_TEST_CN Overall": 62.0,
    "MMBench_TEST_EN_V11 Overall": 69.2,
    "MMBench_TEST_CN_V11 Overall": 59.4,
    "MME Overall": 1935.8,
    "MMVet Overall": 47.4,
    "MMMU_VAL Overall": 41.8,
    "MathVista Overall": 34.7,
    "HallusionBench Overall": 35.8,
    "LLaVABench Overall": 75.8,
    "AI2D Overall": 63.3,
    "ScienceQA_VAL Overall": 78.5,
    "ScienceQA_TEST Overall": 81.1,
    "MMStar Overall": 39.6,
    "RealWorldQA Overall": 58.4,
    "TextVQA_VAL Overall": 62.3,
    "ChartQA_TEST Overall": 22.7,
    "OCRVQA_TESTCORE Overall": 59.1,
    "POPE Overall": 81.7,
    "SEEDBench2_Plus Overall": 46.1,
    "MMT-Bench_VAL Overall": 55.3
  },
  {
    "Model": "DeepSeek-VL-1.3B",
    "Source": "https://github.com/deepseek-ai/DeepSeek-VL",
    "Parameters": "2.0B",
    "Time": "2024/03/21",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 66.0,
    "CCBench Overall": 37.6,
    "MMBench_TEST_EN Overall": 66.4,
    "MMBench_TEST_CN Overall": 62.9,
    "MMBench_TEST_EN_V11 Overall": 65.6,
    "MMBench_TEST_CN_V11 Overall": 61.9,
    "MME Overall": 1531.6,
    "MMVet Overall": 29.2,
    "MMMU_VAL Overall": 33.8,
    "MathVista Overall": 29.8,
    "HallusionBench Overall": 27.6,
    "LLaVABench Overall": 51.1,
    "AI2D Overall": 51.5,
    "ScienceQA_VAL Overall": 64.2,
    "ScienceQA_TEST Overall": 68.4,
    "MMStar Overall": 39.9,
    "RealWorldQA Overall": 49.7,
    "TextVQA_VAL Overall": 57.8,
    "ChartQA_TEST Overall": 47.4,
    "OCRVQA_TESTCORE Overall": 58.1,
    "POPE Overall": 85.9,
    "SEEDBench2_Plus Overall": 43.7,
    "MMT-Bench_VAL Overall": 49.1,
    "BLINK Overall": 39.7
  },
  {
    "Model": "DeepSeek-VL-7B",
    "Source": "https://github.com/deepseek-ai/DeepSeek-VL",
    "Parameters": "7.3B",
    "Time": "2024/03/21",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 70.1,
    "CCBench Overall": 52.4,
    "MMBench_TEST_EN Overall": 73.8,
    "MMBench_TEST_CN Overall": 71.4,
    "MMBench_TEST_EN_V11 Overall": 71.7,
    "MMBench_TEST_CN_V11 Overall": 69.7,
    "MME Overall": 1765.4,
    "MMVet Overall": 39.7,
    "MMMU_VAL Overall": 38.3,
    "MathVista Overall": 36.9,
    "HallusionBench Overall": 34.5,
    "LLaVABench Overall": 77.8,
    "AI2D Overall": 65.3,
    "ScienceQA_VAL Overall": 76.0,
    "ScienceQA_TEST Overall": 80.9,
    "MMStar Overall": 40.5,
    "RealWorldQA Overall": 54.2,
    "TextVQA_VAL Overall": 64.9,
    "ChartQA_TEST Overall": 59.1,
    "OCRVQA_TESTCORE Overall": 67.0,
    "POPE Overall": 85.6,
    "SEEDBench2_Plus Overall": 53.1,
    "MMT-Bench_VAL Overall": 53.5,
    "BLINK Overall": 40.9
  },
  {
    "Model": "XVERSE-V-13B",
    "Source": "https://github.com/xverse-ai/XVERSE-V-13B",
    "Parameters": "13B",
    "Time": "2024/05/10",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 72.4,
    "CCBench Overall": 46.9,
    "MMBench_TEST_EN Overall": 75.4,
    "MMBench_TEST_CN Overall": 74.6,
    "MMBench_TEST_EN_V11 Overall": 72.1,
    "MMBench_TEST_CN_V11 Overall": 60.5,
    "MME Overall": 1909.9,
    "MMVet Overall": 37.8,
    "MMMU_VAL Overall": 44.1,
    "MathVista Overall": 45.3,
    "HallusionBench Overall": 33.3,
    "LLaVABench Overall": 62.9,
    "AI2D Overall": 70.6,
    "ScienceQA_VAL Overall": 86.6,
    "ScienceQA_TEST Overall": 86.8,
    "MMStar Overall": 49.3,
    "RealWorldQA Overall": 60.8,
    "POPE Overall": 29.3,
    "SEEDBench2_Plus Overall": 35.3,
    "MMT-Bench_VAL Overall": 29.3
  },
  {
    "Model": "MiniCPM-Llama3-V2.5",
    "Source": "https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5",
    "Parameters": "8B",
    "Time": "2024/05/27",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 72.3,
    "CCBench Overall": 45.9,
    "MMBench_TEST_EN Overall": 77.6,
    "MMBench_TEST_CN Overall": 73.8,
    "MMBench_TEST_EN_V11 Overall": 74.0,
    "MMBench_TEST_CN_V11 Overall": 70.1,
    "MME Overall": 2024.6,
    "MMVet Overall": 52.8,
    "MMMU_VAL Overall": 45.8,
    "MathVista Overall": 54.3,
    "HallusionBench Overall": 42.4,
    "LLaVABench Overall": 86.7,
    "AI2D Overall": 78.4,
    "ScienceQA_VAL Overall": 88.4,
    "ScienceQA_TEST Overall": 89.2,
    "MMStar Overall": 51.8,
    "RealWorldQA Overall": 63.5,
    "POPE Overall": 86.7,
    "SEEDBench2_Plus Overall": 61.4,
    "MMT-Bench_VAL Overall": 57.6
  },
  {
    "Model": "CogVLM-17B-Chat",
    "Source": "https://huggingface.co/THUDM/cogvlm-chat-hf",
    "Parameters": "17B",
    "Time": "2024/01/03",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 68.8,
    "CCBench Overall": 19.0,
    "MMBench_TEST_EN Overall": 65.8,
    "MMBench_TEST_CN Overall": 55.9,
    "MMBench_TEST_EN_V11 Overall": 63.6,
    "MMBench_TEST_CN_V11 Overall": 54.0,
    "MME Overall": 1736.6,
    "MMVet Overall": 54.5,
    "MMMU_VAL Overall": 37.3,
    "MathVista Overall": 35.0,
    "HallusionBench Overall": 35.4,
    "LLaVABench Overall": 73.9,
    "AI2D Overall": 63.3,
    "ScienceQA_VAL Overall": 65.6,
    "ScienceQA_TEST Overall": 66.2,
    "MMStar Overall": 39.9,
    "RealWorldQA Overall": 60.3,
    "TextVQA_VAL Overall": 78.2,
    "ChartQA_TEST Overall": 65.5,
    "OCRVQA_TESTCORE Overall": 66.3,
    "POPE Overall": 88.0,
    "SEEDBench2_Plus Overall": 54.6,
    "MMT-Bench_VAL Overall": 50.8
  },
  {
    "Model": "CogVLM2-19B-Chat",
    "Source": "https://huggingface.co/THUDM/cogvlm2-llama3-chat-19B",
    "Parameters": "19B",
    "Time": "2024/05/31",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 74.1,
    "CCBench Overall": 38.8,
    "MMBench_TEST_EN Overall": 73.9,
    "MMBench_TEST_CN Overall": 69.8,
    "MMBench_TEST_EN_V11 Overall": 72.7,
    "MMBench_TEST_CN_V11 Overall": 68.6,
    "MME Overall": 1869.5,
    "MMVet Overall": 57.8,
    "MMMU_VAL Overall": 42.6,
    "MathVista Overall": 38.6,
    "HallusionBench Overall": 41.3,
    "LLaVABench Overall": 83.0,
    "AI2D Overall": 73.4,
    "ScienceQA_VAL Overall": 88.4,
    "ScienceQA_TEST Overall": 90.2,
    "MMStar Overall": 50.5,
    "RealWorldQA Overall": 62.9,
    "ChartQA_TEST Overall": 33.0,
    "OCRVQA_TESTCORE Overall": 64.9,
    "POPE Overall": 83.4,
    "SEEDBench2_Plus Overall": 66.0,
    "MMT-Bench_VAL Overall": 54.0
  },
  {
    "Model": "GLM-4v-9B",
    "Source": "https://huggingface.co/THUDM/glm-4v-9b",
    "Parameters": "9B",
    "Time": "2024/06/14",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 71.1,
    "CCBench Overall": 44.9,
    "MMBench_TEST_EN Overall": 71.4,
    "MMBench_TEST_CN Overall": 70.4,
    "MMBench_TEST_EN_V11 Overall": 68.7,
    "MMBench_TEST_CN_V11 Overall": 67.1,
    "MME Overall": 2018.8,
    "MMVet Overall": 58.0,
    "MMMU_VAL Overall": 46.9,
    "MathVista Overall": 51.1,
    "HallusionBench Overall": 45.0,
    "LLaVABench Overall": 92.1,
    "AI2D Overall": 71.2,
    "ScienceQA_VAL Overall": 98.7,
    "ScienceQA_TEST Overall": 96.7,
    "MMStar Overall": 54.8,
    "RealWorldQA Overall": 66.0,
    "POPE Overall": 89.4,
    "SEEDBench2 Overall": 49.4,
    "SEEDBench2_Plus Overall": 59.7,
    "MMT-Bench_VAL Overall": 48.8
  },
  {
    "Model": "IDEFICS-9B-Instruct",
    "Source": "https://huggingface.co/HuggingFaceM4/idefics-9b-instruct",
    "Parameters": "9B",
    "Time": "2023/7/25",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 45.0,
    "CCBench Overall": 7.8,
    "MMBench_TEST_EN Overall": 45.3,
    "MMBench_TEST_CN Overall": 25.2,
    "MMBench_TEST_EN_V11 Overall": 35.2,
    "MMBench_TEST_CN_V11 Overall": 19.5,
    "MME Overall": 1177.3,
    "MMVet Overall": 30.0,
    "MMMU_VAL Overall": 18.4,
    "MathVista Overall": 21.1,
    "HallusionBench Overall": 27.3,
    "LLaVABench Overall": 45.0,
    "AI2D Overall": 42.2,
    "ScienceQA_VAL Overall": 51.6,
    "ScienceQA_TEST Overall": 53.5,
    "MMStar Overall": 21.6,
    "RealWorldQA Overall": 42.1,
    "POPE Overall": 81.9,
    "SEEDBench2_Plus Overall": 35.4,
    "MMT-Bench_VAL Overall": 45.3,
    "BLINK Overall": 38.3
  },
  {
    "Model": "IDEFICS-80B-Instruct",
    "Source": "https://huggingface.co/HuggingFaceM4/idefics-80b-instruct",
    "Parameters": "80B",
    "Time": "2023/7/25",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 52.0,
    "CCBench Overall": 10.6,
    "MMBench_TEST_EN Overall": 54.6,
    "MMBench_TEST_CN Overall": 38.1,
    "MMBench_TEST_EN_V11 Overall": 11.3,
    "MMBench_TEST_CN_V11 Overall": 44.4,
    "MME Overall": 1518.2,
    "MMVet Overall": 39.7,
    "MMMU_VAL Overall": 24.0,
    "MathVista Overall": 26.8,
    "HallusionBench Overall": 23.4,
    "LLaVABench Overall": 56.9,
    "AI2D Overall": 54.8,
    "ScienceQA_VAL Overall": 59.9,
    "ScienceQA_TEST Overall": 61.8,
    "MMStar Overall": 26.1,
    "RealWorldQA Overall": 49.0,
    "POPE Overall": 66.0,
    "SEEDBench2_Plus Overall": 41.5
  },
  {
    "Model": "IDEFICS2-8B",
    "Source": "https://huggingface.co/HuggingFaceM4/idefics2-8b",
    "Parameters": "8B",
    "Time": "2024/04/26",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 71.9,
    "CCBench Overall": 37.6,
    "MMBench_TEST_EN Overall": 75.7,
    "MMBench_TEST_CN Overall": 68.6,
    "MMBench_TEST_EN_V11 Overall": 72.7,
    "MMBench_TEST_CN_V11 Overall": 65.1,
    "MME Overall": 1847.6,
    "MMVet Overall": 34.0,
    "MMMU_VAL Overall": 45.2,
    "MathVista Overall": 52.2,
    "HallusionBench Overall": 39.1,
    "LLaVABench Overall": 49.1,
    "AI2D Overall": 72.3,
    "ScienceQA_VAL Overall": 86.5,
    "ScienceQA_TEST Overall": 88.7,
    "MMStar Overall": 49.5,
    "RealWorldQA Overall": 60.7,
    "TextVQA_VAL Overall": 68.9,
    "POPE Overall": 86.2,
    "SEEDBench2 Overall": 53.4,
    "SEEDBench2_Plus Overall": 56.6,
    "MMT-Bench_VAL Overall": 56.8,
    "BLINK Overall": 45.6
  },
  {
    "Model": "InternVL-Chat-V1.5",
    "Source": "https://github.com/OpenGVLab/InternVL",
    "Parameters": "26B",
    "Time": "2024/04/17",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 75.2,
    "CCBench Overall": 69.8,
    "MMBench_TEST_EN Overall": 82.3,
    "MMBench_TEST_CN Overall": 80.7,
    "MMBench_TEST_EN_V11 Overall": 80.3,
    "MMBench_TEST_CN_V11 Overall": 79.1,
    "MME Overall": 2189.6,
    "MMVet Overall": 55.4,
    "MMMU_VAL Overall": 46.8,
    "MathVista Overall": 54.7,
    "HallusionBench Overall": 47.4,
    "LLaVABench Overall": 83.3,
    "AI2D Overall": 80.6,
    "ScienceQA_VAL Overall": 93.7,
    "ScienceQA_TEST Overall": 93.9,
    "MMStar Overall": 57.1,
    "RealWorldQA Overall": 65.6,
    "TextVQA_VAL Overall": 80.4,
    "ChartQA_TEST Overall": 83.9,
    "OCRVQA_TESTCORE Overall": 64.2,
    "POPE Overall": 87.5,
    "SEEDBench2 Overall": 59.1,
    "SEEDBench2_Plus Overall": 66.4,
    "MMT-Bench_VAL Overall": 57.8,
    "BLINK Overall": 46.6
  },
  {
    "Model": "Mini-InternVL-Chat-2B-V1.5",
    "Source": "https://github.com/OpenGVLab/InternVL",
    "Parameters": "2B",
    "Time": "2024/06/10",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 69.5,
    "CCBench Overall": 62.9,
    "MMBench_TEST_EN Overall": 70.7,
    "MMBench_TEST_CN Overall": 66.0,
    "MMBench_TEST_EN_V11 Overall": 67.5,
    "MMBench_TEST_CN_V11 Overall": 63.0,
    "MME Overall": 1906.1,
    "MMVet Overall": 35.5,
    "MMMU_VAL Overall": 37.4,
    "MathVista Overall": 41.3,
    "HallusionBench Overall": 37.3,
    "LLaVABench Overall": 57.3,
    "AI2D Overall": 69.7,
    "ScienceQA_VAL Overall": 81.9,
    "ScienceQA_TEST Overall": 84.8,
    "MMStar Overall": 46.7,
    "RealWorldQA Overall": 57.9,
    "POPE Overall": 85.4,
    "SEEDBench2 Overall": 54.0,
    "SEEDBench2_Plus Overall": 59.1,
    "MMT-Bench_VAL Overall": 48.9
  },
  {
    "Model": "Mini-InternVL-Chat-4B-V1.5",
    "Source": "https://github.com/OpenGVLab/InternVL",
    "Parameters": "4B",
    "Time": "2024/06/10",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 72.2,
    "CCBench Overall": 59.4,
    "MMBench_TEST_EN Overall": 75.7,
    "MMBench_TEST_CN Overall": 70.0,
    "MMBench_TEST_EN_V11 Overall": 72.6,
    "MMBench_TEST_CN_V11 Overall": 66.8,
    "MME Overall": 2079.2,
    "MMVet Overall": 43.6,
    "MMMU_VAL Overall": 45.1,
    "MathVista Overall": 54.6,
    "HallusionBench Overall": 43.0,
    "LLaVABench Overall": 68.6,
    "AI2D Overall": 77.0,
    "ScienceQA_VAL Overall": 92.1,
    "ScienceQA_TEST Overall": 92.6,
    "MMStar Overall": 53.1,
    "RealWorldQA Overall": 60.8,
    "POPE Overall": 81.4,
    "SEEDBench2 Overall": 57.3,
    "SEEDBench2_Plus Overall": 62.5,
    "MMT-Bench_VAL Overall": 57.8
  },
  {
    "Model": "PaliGemma-3B-mix-448",
    "Source": "https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md",
    "Parameters": "3B",
    "Time": "2024/05/17",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 69.6,
    "CCBench Overall": 29.6,
    "MMBench_TEST_EN Overall": 71.0,
    "MMBench_TEST_CN Overall": 63.6,
    "MMBench_TEST_EN_V11 Overall": 68.4,
    "MMBench_TEST_CN_V11 Overall": 62.8,
    "MME Overall": 1686.1,
    "MMVet Overall": 33.1,
    "MMMU_VAL Overall": 34.9,
    "MathVista Overall": 28.7,
    "HallusionBench Overall": 32.2,
    "LLaVABench Overall": 36.9,
    "AI2D Overall": 68.3,
    "ScienceQA_VAL Overall": 93.3,
    "ScienceQA_TEST Overall": 94.3,
    "MMStar Overall": 48.3,
    "RealWorldQA Overall": 55.2,
    "TextVQA_VAL Overall": 68.1,
    "ChartQA_TEST Overall": 33.7,
    "OCRVQA_TESTCORE Overall": 57.8,
    "POPE Overall": 87.5,
    "SEEDBench2_Plus Overall": 49.8,
    "MMT-Bench_VAL Overall": 46.9,
    "BLINK Overall": 40.7
  },
  {
    "Model": "Cambrian-8B",
    "Source": "https://huggingface.co/nyu-visionx/cambrian-8b",
    "Parameters": "8B",
    "Time": "2024/07/11",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 73.3,
    "CCBench Overall": 23.7,
    "MMBench_TEST_EN Overall": 74.6,
    "MMBench_TEST_CN Overall": 67.9,
    "MMBench_TEST_EN_V11 Overall": 71.6,
    "MMBench_TEST_CN_V11 Overall": 64.9,
    "MME Overall": 1802.9,
    "MMVet Overall": 48.0,
    "MMMU_VAL Overall": 41.8,
    "MathVista Overall": 47.0,
    "HallusionBench Overall": 30.6,
    "LLaVABench Overall": 71.0,
    "AI2D Overall": 74.6,
    "ScienceQA_VAL Overall": 80.5,
    "ScienceQA_TEST Overall": 81.0,
    "MMStar Overall": 50.7,
    "RealWorldQA Overall": 60.0,
    "TextVQA_VAL Overall": 72.6,
    "ChartQA_TEST Overall": 72.6,
    "OCRVQA_TESTCORE Overall": 66.0,
    "POPE Overall": 86.4,
    "SEEDBench2_Plus Overall": 59.7,
    "BLINK Overall": 44.9
  },
  {
    "Model": "Cambrian-13B",
    "Source": "https://huggingface.co/nyu-visionx/cambrian-13b",
    "Parameters": "13B",
    "Time": "2024/07/29",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 73.2,
    "CCBench Overall": 26.7,
    "MMBench_TEST_EN Overall": 73.2,
    "MMBench_TEST_CN Overall": 65.0,
    "MMBench_TEST_EN_V11 Overall": 71.4,
    "MMBench_TEST_CN_V11 Overall": 63.6,
    "MME Overall": 1876.8,
    "MMVet Overall": 48.9,
    "MMMU_VAL Overall": 41.6,
    "MathVista Overall": 47.4,
    "HallusionBench Overall": 39.4,
    "LLaVABench Overall": 76.1,
    "AI2D Overall": 73.6,
    "ScienceQA_VAL Overall": 76.4,
    "ScienceQA_TEST Overall": 79.3,
    "MMStar Overall": 47.1,
    "RealWorldQA Overall": 58.6,
    "POPE Overall": 86.8,
    "SEEDBench2_Plus Overall": 60.0,
    "BLINK Overall": 43.1
  },
  {
    "Model": "Cambrian-34B",
    "Source": "https://huggingface.co/nyu-visionx/cambrian-34b",
    "Parameters": "34B",
    "Time": "2024/07/18",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 75.4,
    "CCBench Overall": 49.2,
    "MMBench_TEST_EN Overall": 80.4,
    "MMBench_TEST_CN Overall": 79.2,
    "MMBench_TEST_EN_V11 Overall": 78.3,
    "MMBench_TEST_CN_V11 Overall": 77.3,
    "MME Overall": 2049.9,
    "MMVet Overall": 53.2,
    "MMMU_VAL Overall": 50.4,
    "MathVista Overall": 50.3,
    "HallusionBench Overall": 41.6,
    "LLaVABench Overall": 82.0,
    "AI2D Overall": 79.5,
    "ScienceQA_VAL Overall": 85.3,
    "ScienceQA_TEST Overall": 85.6,
    "MMStar Overall": 54.2,
    "RealWorldQA Overall": 67.1,
    "TextVQA_VAL Overall": 72.1,
    "ChartQA_TEST Overall": 73.7,
    "OCRVQA_TESTCORE Overall": 68.2,
    "POPE Overall": 85.1,
    "SEEDBench2_Plus Overall": 65.1
  },
  {
    "Model": "Chameleon-7B",
    "Source": "https://huggingface.co/facebook/chameleon-7b",
    "Parameters": "7B",
    "Time": "2024/07/29",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 30.5,
    "CCBench Overall": 1.4,
    "MMBench_TEST_EN Overall": 15.4,
    "MMBench_TEST_CN Overall": 7.1,
    "MMBench_TEST_EN_V11 Overall": 19.9,
    "MMBench_TEST_CN_V11 Overall": 19.6,
    "MME Overall": 202.7,
    "MMVet Overall": 8.3,
    "MMMU_VAL Overall": 22.4,
    "MathVista Overall": 22.3,
    "HallusionBench Overall": 17.1,
    "LLaVABench Overall": 26.6,
    "AI2D Overall": 46.0,
    "ScienceQA_VAL Overall": 44.6,
    "ScienceQA_TEST Overall": 46.8,
    "MMStar Overall": 31.1,
    "RealWorldQA Overall": 39.0,
    "POPE Overall": 19.4,
    "SEEDBench2_Plus Overall": 29.7,
    "MMT-Bench_VAL Overall": 23.9,
    "BLINK Overall": 16.6
  },
  {
    "Model": "Chameleon-30B",
    "Source": "https://huggingface.co/facebook/chameleon-30b",
    "Parameters": "30B",
    "Time": "2024/07/29",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 48.5,
    "CCBench Overall": 5.9,
    "MMBench_TEST_EN Overall": 32.5,
    "MMBench_TEST_CN Overall": 17.5,
    "MMBench_TEST_EN_V11 Overall": 37.6,
    "MMBench_TEST_CN_V11 Overall": 27.8,
    "MME Overall": 604.5,
    "MMVet Overall": 9.7,
    "MMMU_VAL Overall": 38.8,
    "MathVista Overall": 23.6,
    "HallusionBench Overall": 18.6,
    "LLaVABench Overall": 33.3,
    "AI2D Overall": 53.7,
    "ScienceQA_VAL Overall": 57.6,
    "ScienceQA_TEST Overall": 58.8,
    "MMStar Overall": 31.8,
    "RealWorldQA Overall": 39.2,
    "POPE Overall": 59.8,
    "SEEDBench2_Plus Overall": 39.4,
    "MMT-Bench_VAL Overall": 32.4,
    "BLINK Overall": 21.3
  },
  {
    "Model": "InternLM-XComposer",
    "Source": "https://github.com/InternLM/InternLM-XComposer",
    "Parameters": "8B",
    "Time": "2023/9/26",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 66.1,
    "CCBench Overall": 49.6,
    "MMBench_TEST_EN Overall": 74.4,
    "MMBench_TEST_CN Overall": 72.4,
    "MMBench_TEST_EN_V11 Overall": 6.4,
    "MMBench_TEST_CN_V11 Overall": 45.5,
    "MME Overall": 1874.2,
    "MMVet Overall": 35.2,
    "MMMU_VAL Overall": 35.6,
    "MathVista Overall": 29.8,
    "HallusionBench Overall": 36.0,
    "LLaVABench Overall": 53.8,
    "AI2D Overall": 56.9,
    "ScienceQA_VAL Overall": 88.0,
    "ScienceQA_TEST Overall": 89.8,
    "MMStar Overall": 6.9,
    "RealWorldQA Overall": 10.2,
    "TextVQA_VAL Overall": 38.5,
    "OCRVQA_TESTCORE Overall": 54.5,
    "POPE Overall": 70.9,
    "SEEDBench2_Plus Overall": 3.6,
    "MMT-Bench_VAL Overall": 3.2,
    "BLINK Overall": 17.8
  },
  {
    "Model": "InternLM-XComposer2",
    "Source": "https://github.com/InternLM/InternLM-XComposer",
    "Parameters": "7B",
    "Time": "2024/01/26",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 74.9,
    "CCBench Overall": 48.2,
    "MMBench_TEST_EN Overall": 80.7,
    "MMBench_TEST_CN Overall": 79.4,
    "MMBench_TEST_EN_V11 Overall": 78.1,
    "MMBench_TEST_CN_V11 Overall": 77.1,
    "MME Overall": 2220.4,
    "MMVet Overall": 46.7,
    "MMMU_VAL Overall": 41.4,
    "MathVista Overall": 59.5,
    "HallusionBench Overall": 41.0,
    "LLaVABench Overall": 72.6,
    "AI2D Overall": 81.2,
    "ScienceQA_VAL Overall": 96.5,
    "ScienceQA_TEST Overall": 96.7,
    "MMStar Overall": 56.2,
    "RealWorldQA Overall": 63.8,
    "TextVQA_VAL Overall": 69.7,
    "ChartQA_TEST Overall": 71.8,
    "OCRVQA_TESTCORE Overall": 62.9,
    "POPE Overall": 83.0,
    "SEEDBench2_Plus Overall": 58.0,
    "MMT-Bench_VAL Overall": 55.7,
    "BLINK Overall": 47.5
  },
  {
    "Model": "InternLM-XComposer2-1.8B",
    "Source": "https://github.com/InternLM/InternLM-XComposer",
    "Parameters": "2B",
    "Time": "2024/06/09",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 70.8,
    "CCBench Overall": 40.4,
    "MMBench_TEST_EN Overall": 73.0,
    "MMBench_TEST_CN Overall": 68.4,
    "MMBench_TEST_EN_V11 Overall": 70.4,
    "MMBench_TEST_CN_V11 Overall": 66.8,
    "MME Overall": 1895.9,
    "MMVet Overall": 31.2,
    "MMMU_VAL Overall": 29.7,
    "MathVista Overall": 50.1,
    "HallusionBench Overall": 33.7,
    "LLaVABench Overall": 55.7,
    "AI2D Overall": 71.1,
    "ScienceQA_VAL Overall": 91.9,
    "ScienceQA_TEST Overall": 92.2,
    "MMStar Overall": 46.4,
    "RealWorldQA Overall": 56.3,
    "POPE Overall": 84.2,
    "SEEDBench2_Plus Overall": 50.8,
    "MMT-Bench_VAL Overall": 48.6,
    "BLINK Overall": 41.5
  },
  {
    "Model": "InternLM-XComposer2-4KHD",
    "Source": "https://github.com/InternLM/InternLM-XComposer",
    "Parameters": "7B",
    "Time": "2024/06/09",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 74.6,
    "CCBench Overall": 47.1,
    "MMBench_TEST_EN Overall": 80.2,
    "MMBench_TEST_CN Overall": 78.1,
    "MMBench_TEST_EN_V11 Overall": 77.0,
    "MMBench_TEST_CN_V11 Overall": 76.1,
    "MME Overall": 2130.4,
    "MMVet Overall": 48.2,
    "MMMU_VAL Overall": 39.7,
    "MathVista Overall": 59.4,
    "HallusionBench Overall": 42.5,
    "LLaVABench Overall": 61.8,
    "AI2D Overall": 81.0,
    "ScienceQA_VAL Overall": 96.3,
    "ScienceQA_TEST Overall": 96.3,
    "MMStar Overall": 55.3,
    "RealWorldQA Overall": 63.3,
    "POPE Overall": 2.9,
    "SEEDBench2 Overall": 57.4,
    "SEEDBench2_Plus Overall": 65.2,
    "MMT-Bench_VAL Overall": 56.3,
    "BLINK Overall": 48.9
  },
  {
    "Model": "InternLM-XComposer2.5",
    "Source": "https://huggingface.co/internlm/internlm-xcomposer2d5-7b",
    "Parameters": "8B",
    "Time": "2024/07/11",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 75.4,
    "CCBench Overall": 49.6,
    "MMBench_TEST_EN Overall": 82.0,
    "MMBench_TEST_CN Overall": 80.9,
    "MMBench_TEST_EN_V11 Overall": 80.1,
    "MMBench_TEST_CN_V11 Overall": 78.8,
    "MME Overall": 2233.1,
    "MMVet Overall": 49.3,
    "MMMU_VAL Overall": 42.9,
    "MathVista Overall": 63.7,
    "HallusionBench Overall": 43.1,
    "LLaVABench Overall": 70.2,
    "AI2D Overall": 81.6,
    "ScienceQA_VAL Overall": 96.1,
    "ScienceQA_TEST Overall": 96.6,
    "MMStar Overall": 59.9,
    "RealWorldQA Overall": 67.8,
    "TextVQA_VAL Overall": 78.2,
    "POPE Overall": 87.9,
    "SEEDBench2_Plus Overall": 66.5,
    "MMT-Bench_VAL Overall": 57.1
  },
  {
    "Model": "GPT-4v (1106, detail-low)",
    "Source": "https://openai.com/research/gpt-4v-system-card",
    "Parameters": "",
    "Time": "2023/12/23",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 71.6,
    "CCBench Overall": 46.5,
    "MMBench_TEST_EN Overall": 77.0,
    "MMBench_TEST_CN Overall": 74.4,
    "MMBench_TEST_EN_V11 Overall": 75.3,
    "MMBench_TEST_CN_V11 Overall": 72.2,
    "MME Overall": 1771.5,
    "MMVet Overall": 56.8,
    "MMMU_VAL Overall": 53.8,
    "MathVista Overall": 48.7,
    "HallusionBench Overall": 46.5,
    "LLaVABench Overall": 93.1,
    "AI2D Overall": 75.9,
    "ScienceQA_VAL Overall": 84.6,
    "ScienceQA_TEST Overall": 82.1,
    "MMStar Overall": 49.7,
    "RealWorldQA Overall": 56.5,
    "POPE Overall": 75.4
  },
  {
    "Model": "GPT-4v (1106, detail-high)",
    "Source": "https://openai.com/research/gpt-4v-system-card",
    "Parameters": "",
    "Time": "2024/04/22",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 72.3,
    "CCBench Overall": 50.4,
    "MMBench_TEST_EN Overall": 65.4,
    "MMBench_TEST_CN Overall": 63.3,
    "MMBench_TEST_EN_V11 Overall": 65.4,
    "MMBench_TEST_CN_V11 Overall": 65.5,
    "MME Overall": 1790.3,
    "MMVet Overall": 49.0,
    "MMMU_VAL Overall": 59.3,
    "MathVista Overall": 48.2,
    "HallusionBench Overall": 39.3,
    "LLaVABench Overall": 88.0,
    "AI2D Overall": 71.4,
    "MMStar Overall": 50.4,
    "RealWorldQA Overall": 63.0
  },
  {
    "Model": "GPT-4v (0409, detail-low)",
    "Source": "https://openai.com/research/gpt-4v-system-card",
    "Parameters": "",
    "Time": "2024/05/15",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 72.0,
    "CCBench Overall": 55.5,
    "MMBench_TEST_EN Overall": 80.8,
    "MMBench_TEST_CN Overall": 79.1,
    "MMBench_TEST_EN_V11 Overall": 79.3,
    "MMBench_TEST_CN_V11 Overall": 78.2,
    "MME Overall": 2037.8,
    "MMVet Overall": 63.6,
    "MMMU_VAL Overall": 62.3,
    "MathVista Overall": 51.4,
    "HallusionBench Overall": 41.6,
    "LLaVABench Overall": 95.3,
    "AI2D Overall": 76.1,
    "MMStar Overall": 52.9,
    "RealWorldQA Overall": 61.4
  },
  {
    "Model": "GPT-4v (0409, detail-high)",
    "Source": "https://openai.com/research/gpt-4v-system-card",
    "Parameters": "",
    "Time": "2024/05/15",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 73.0,
    "CCBench Overall": 57.3,
    "MMBench_TEST_EN Overall": 81.0,
    "MMBench_TEST_CN Overall": 80.2,
    "MMBench_TEST_EN_V11 Overall": 80.0,
    "MMBench_TEST_CN_V11 Overall": 79.5,
    "MME Overall": 2070.2,
    "MMVet Overall": 67.5,
    "MMMU_VAL Overall": 61.7,
    "MathVista Overall": 54.7,
    "HallusionBench Overall": 43.9,
    "LLaVABench Overall": 99.8,
    "AI2D Overall": 78.6,
    "MMStar Overall": 56.0,
    "RealWorldQA Overall": 68.0
  },
  {
    "Model": "GPT-4o (0513, detail-low)",
    "Source": "https://openai.com/index/hello-gpt-4o/",
    "Parameters": "",
    "Time": "2024/05/15",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 76.4,
    "CCBench Overall": 70.6,
    "MMBench_TEST_EN Overall": 83.3,
    "MMBench_TEST_CN Overall": 82.1,
    "MMBench_TEST_EN_V11 Overall": 83.1,
    "MMBench_TEST_CN_V11 Overall": 82.5,
    "MME Overall": 2328.7,
    "MMVet Overall": 66.5,
    "MMMU_VAL Overall": 62.8,
    "MathVista Overall": 56.5,
    "HallusionBench Overall": 51.7,
    "LLaVABench Overall": 97.2,
    "AI2D Overall": 77.4,
    "MMStar Overall": 61.6,
    "RealWorldQA Overall": 68.6,
    "POPE Overall": 85.0,
    "SEEDBench2_Plus Overall": 64.0,
    "MMT-Bench_VAL Overall": 65.6,
    "BLINK Overall": 64.2
  },
  {
    "Model": "GPT-4o (0513, detail-high)",
    "Source": "https://openai.com/index/hello-gpt-4o/",
    "Parameters": "",
    "Time": "2024/05/31",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 77.1,
    "CCBench Overall": 71.2,
    "MMBench_TEST_EN Overall": 83.4,
    "MMBench_TEST_CN Overall": 82.1,
    "MMBench_TEST_EN_V11 Overall": 83.0,
    "MMBench_TEST_CN_V11 Overall": 81.5,
    "MME Overall": 2310.3,
    "MMVet Overall": 69.1,
    "MMMU_VAL Overall": 69.2,
    "MathVista Overall": 61.3,
    "HallusionBench Overall": 55.0,
    "LLaVABench Overall": 102.0,
    "AI2D Overall": 84.6,
    "ScienceQA_VAL Overall": 89.7,
    "ScienceQA_TEST Overall": 90.7,
    "MMStar Overall": 63.9,
    "RealWorldQA Overall": 75.4,
    "POPE Overall": 85.6,
    "SEEDBench2_Plus Overall": 72.0,
    "MMT-Bench_VAL Overall": 67.3,
    "BLINK Overall": 68.0
  },
  {
    "Model": "GPT-4o-mini (0718, detail-high)",
    "Source": "https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/",
    "Parameters": "",
    "Time": "2024/07/20",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 72.8,
    "CCBench Overall": 61.4,
    "MMBench_TEST_EN Overall": 77.6,
    "MMBench_TEST_CN Overall": 75.9,
    "MMBench_TEST_EN_V11 Overall": 77.1,
    "MMBench_TEST_CN_V11 Overall": 75.0,
    "MME Overall": 2003.4,
    "MMVet Overall": 66.9,
    "MMMU_VAL Overall": 60.0,
    "MathVista Overall": 52.4,
    "HallusionBench Overall": 46.1,
    "LLaVABench Overall": 98.7,
    "AI2D Overall": 77.8,
    "ScienceQA_VAL Overall": 85.6,
    "ScienceQA_TEST Overall": 85.4,
    "MMStar Overall": 54.8,
    "RealWorldQA Overall": 67.1,
    "POPE Overall": 81.9,
    "SEEDBench2_Plus Overall": 61.4,
    "MMT-Bench_VAL Overall": 61.2,
    "BLINK Overall": 53.6
  },
  {
    "Model": "Gemini-1.0-Pro",
    "Source": "https://deepmind.google/technologies/gemini/",
    "Parameters": "",
    "Time": "2023/12/23",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 70.7,
    "CCBench Overall": 52.5,
    "MMBench_TEST_EN Overall": 73.6,
    "MMBench_TEST_CN Overall": 74.3,
    "MMBench_TEST_EN_V11 Overall": 70.2,
    "MMBench_TEST_CN_V11 Overall": 69.2,
    "MME Overall": 2148.9,
    "MMVet Overall": 58.6,
    "MMMU_VAL Overall": 49.0,
    "MathVista Overall": 46.5,
    "HallusionBench Overall": 45.7,
    "LLaVABench Overall": 79.9,
    "AI2D Overall": 72.9,
    "ScienceQA_VAL Overall": 80.1,
    "ScienceQA_TEST Overall": 81.4,
    "MMStar Overall": 38.6,
    "RealWorldQA Overall": 60.4,
    "TextVQA_VAL Overall": 36.9,
    "OCRVQA_TESTCORE Overall": 12.3
  },
  {
    "Model": "Gemini-1.5-Pro",
    "Source": "https://deepmind.google/technologies/gemini/",
    "Parameters": "",
    "Time": "2024/06/10",
    "OpenSource": "No",
    "CCBench Overall": 28.4,
    "MMBench_TEST_EN Overall": 73.9,
    "MMBench_TEST_CN Overall": 73.8,
    "MMBench_TEST_EN_V11 Overall": 74.6,
    "MMBench_TEST_CN_V11 Overall": 73.2,
    "MME Overall": 2110.6,
    "MMVet Overall": 64.0,
    "MMMU_VAL Overall": 60.6,
    "MathVista Overall": 57.7,
    "HallusionBench Overall": 45.6,
    "LLaVABench Overall": 95.3,
    "AI2D Overall": 79.1,
    "ScienceQA_VAL Overall": 85.9,
    "ScienceQA_TEST Overall": 85.7,
    "MMStar Overall": 59.1,
    "RealWorldQA Overall": 64.1,
    "POPE Overall": 88.2
  },
  {
    "Model": "Qwen-VL-Plus",
    "Source": "https://github.com/QwenLM/Qwen-VL?tab=readme-ov-file#qwen-vl-plus",
    "Parameters": "",
    "Time": "2024/01/10",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 65.7,
    "CCBench Overall": 55.1,
    "MMBench_TEST_EN Overall": 67.0,
    "MMBench_TEST_CN Overall": 70.7,
    "MMBench_TEST_EN_V11 Overall": 64.3,
    "MMBench_TEST_CN_V11 Overall": 68.2,
    "MME Overall": 2229.8,
    "MMVet Overall": 55.7,
    "MMMU_VAL Overall": 39.8,
    "MathVista Overall": 37.6,
    "HallusionBench Overall": 40.6,
    "LLaVABench Overall": 73.7,
    "AI2D Overall": 65.7,
    "ScienceQA_VAL Overall": 71.6,
    "ScienceQA_TEST Overall": 73.4,
    "MMStar Overall": 39.7,
    "RealWorldQA Overall": 44.6,
    "OCRVQA_TESTCORE Overall": 38.8,
    "POPE Overall": 85.4
  },
  {
    "Model": "Qwen-VL-Max",
    "Source": "https://huggingface.co/spaces/Qwen/Qwen-VL-Max",
    "Parameters": "",
    "Time": "2024/02/02",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 72.7,
    "CCBench Overall": 63.5,
    "MMBench_TEST_EN Overall": 77.6,
    "MMBench_TEST_CN Overall": 75.7,
    "MMBench_TEST_EN_V11 Overall": 75.4,
    "MMBench_TEST_CN_V11 Overall": 73.7,
    "MME Overall": 2281.7,
    "MMVet Overall": 61.8,
    "MMMU_VAL Overall": 52.0,
    "MathVista Overall": 43.4,
    "HallusionBench Overall": 41.2,
    "LLaVABench Overall": 82.3,
    "AI2D Overall": 75.7,
    "ScienceQA_VAL Overall": 79.8,
    "ScienceQA_TEST Overall": 80.0,
    "MMStar Overall": 49.5,
    "RealWorldQA Overall": 61.3,
    "OCRVQA_TESTCORE Overall": 55.0,
    "POPE Overall": 71.9,
    "SEEDBench2_Plus Overall": 61.6,
    "MMT-Bench_VAL Overall": 58.3,
    "BLINK Overall": 43.6
  },
  {
    "Model": "Step-1V",
    "Source": "https://platform.stepfun.com",
    "Parameters": "",
    "Time": "2024/03/15",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 70.3,
    "CCBench Overall": 71.2,
    "MMBench_TEST_EN Overall": 80.7,
    "MMBench_TEST_CN Overall": 79.9,
    "MMBench_TEST_EN_V11 Overall": 78.2,
    "MMBench_TEST_CN_V11 Overall": 77.8,
    "MME Overall": 2206.4,
    "MMVet Overall": 63.3,
    "MMMU_VAL Overall": 49.9,
    "MathVista Overall": 44.8,
    "HallusionBench Overall": 48.4,
    "LLaVABench Overall": 78.0,
    "AI2D Overall": 79.2,
    "ScienceQA_VAL Overall": 82.6,
    "ScienceQA_TEST Overall": 84.0,
    "MMStar Overall": 50.0,
    "RealWorldQA Overall": 60.0,
    "TextVQA_VAL Overall": 71.6,
    "ChartQA_TEST Overall": 57.7,
    "OCRVQA_TESTCORE Overall": 62.4,
    "POPE Overall": 86.8
  },
  {
    "Model": "Claude3-Haiku",
    "Source": "https://docs.anthropic.com/claude/docs/vision",
    "Parameters": "",
    "Time": "2024/03/28",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 63.3,
    "CCBench Overall": 24.5,
    "MMBench_TEST_EN Overall": 60.7,
    "MMBench_TEST_CN Overall": 57.2,
    "MMBench_TEST_EN_V11 Overall": 58.0,
    "MMBench_TEST_CN_V11 Overall": 56.2,
    "MME Overall": 1453.2,
    "MMVet Overall": 46.4,
    "MMMU_VAL Overall": 49.7,
    "MathVista Overall": 42.2,
    "HallusionBench Overall": 39.2,
    "LLaVABench Overall": 67.4,
    "AI2D Overall": 65.6,
    "MMStar Overall": 38.1,
    "RealWorldQA Overall": 45.5,
    "POPE Overall": 74.4,
    "SEEDBench2_Plus Overall": 61.7,
    "MMT-Bench_VAL Overall": 50.4,
    "BLINK Overall": 37.5
  },
  {
    "Model": "Claude3-Sonnet",
    "Source": "https://docs.anthropic.com/claude/docs/vision",
    "Parameters": "",
    "Time": "2024/03/28",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 65.0,
    "CCBench Overall": 27.8,
    "MMBench_TEST_EN Overall": 67.8,
    "MMBench_TEST_CN Overall": 64.2,
    "MMBench_TEST_EN_V11 Overall": 64.2,
    "MMBench_TEST_CN_V11 Overall": 63.5,
    "MME Overall": 1625.9,
    "MMVet Overall": 51.7,
    "MMMU_VAL Overall": 47.4,
    "MathVista Overall": 45.0,
    "HallusionBench Overall": 41.3,
    "LLaVABench Overall": 73.2,
    "AI2D Overall": 69.9,
    "MMStar Overall": 44.2,
    "RealWorldQA Overall": 50.7,
    "POPE Overall": 68.2,
    "SEEDBench2_Plus Overall": 27.1,
    "MMT-Bench_VAL Overall": 10.2,
    "BLINK Overall": 39.3
  },
  {
    "Model": "Claude3.5-Sonnet",
    "Source": "https://docs.anthropic.com/claude/docs/vision",
    "Parameters": "",
    "Time": "2024/06/24",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 72.2,
    "CCBench Overall": 54.1,
    "MMBench_TEST_EN Overall": 79.7,
    "MMBench_TEST_CN Overall": 80.7,
    "MMBench_TEST_EN_V11 Overall": 77.7,
    "MMBench_TEST_CN_V11 Overall": 79.3,
    "MME Overall": 1920.0,
    "MMVet Overall": 66.0,
    "MMMU_VAL Overall": 65.9,
    "MathVista Overall": 61.6,
    "HallusionBench Overall": 49.9,
    "LLaVABench Overall": 81.0,
    "AI2D Overall": 80.2,
    "ScienceQA_VAL Overall": 89.3,
    "ScienceQA_TEST Overall": 88.9,
    "MMStar Overall": 62.2,
    "RealWorldQA Overall": 60.1,
    "POPE Overall": 73.6,
    "SEEDBench2_Plus Overall": 71.7,
    "MMT-Bench_VAL Overall": 61.8,
    "BLINK Overall": 28.2
  },
  {
    "Model": "Claude3-Opus",
    "Source": "https://docs.anthropic.com/claude/docs/vision",
    "Parameters": "",
    "Time": "2024/03/28",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 64.0,
    "CCBench Overall": 26.3,
    "MMBench_TEST_EN Overall": 63.3,
    "MMBench_TEST_CN Overall": 59.2,
    "MMBench_TEST_EN_V11 Overall": 60.1,
    "MMBench_TEST_CN_V11 Overall": 58.1,
    "MME Overall": 1586.8,
    "MMVet Overall": 51.7,
    "MMMU_VAL Overall": 54.9,
    "MathVista Overall": 45.8,
    "HallusionBench Overall": 37.8,
    "LLaVABench Overall": 73.9,
    "AI2D Overall": 70.6,
    "MMStar Overall": 45.7,
    "RealWorldQA Overall": 48.4,
    "POPE Overall": 74.0
  },
  {
    "Model": "GLM-4v",
    "Source": "https://open.bigmodel.cn/dev/api#glm-4v",
    "Parameters": "",
    "Time": "2024/05/20",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 75.1,
    "CCBench Overall": 63.9,
    "MMBench_TEST_EN Overall": 81.3,
    "MMBench_TEST_CN Overall": 79.4,
    "MMBench_TEST_EN_V11 Overall": 79.2,
    "MMBench_TEST_CN_V11 Overall": 78.0,
    "MME Overall": 2163.7,
    "MMVet Overall": 60.7,
    "MMMU_VAL Overall": 45.6,
    "MathVista Overall": 45.6,
    "HallusionBench Overall": 44.9,
    "LLaVABench Overall": 87.5,
    "AI2D Overall": 76.1,
    "ScienceQA_VAL Overall": 98.6,
    "ScienceQA_TEST Overall": 97.4,
    "MMStar Overall": 53.2,
    "RealWorldQA Overall": 67.3,
    "POPE Overall": 89.9
  },
  {
    "Model": "CongRong",
    "Source": "https://mllm.cloudwalk.com/web/",
    "Parameters": "",
    "Time": "2024/06/26",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 76.8,
    "CCBench Overall": 69.6,
    "MMBench_TEST_EN Overall": 82.8,
    "MMBench_TEST_CN Overall": 81.9,
    "MMBench_TEST_EN_V11 Overall": 80.9,
    "MMBench_TEST_CN_V11 Overall": 80.4,
    "MME Overall": 2278.3,
    "MMVet Overall": 57.5,
    "MMMU_VAL Overall": 48.3,
    "MathVista Overall": 61.0,
    "HallusionBench Overall": 50.6,
    "LLaVABench Overall": 80.7,
    "AI2D Overall": 82.4,
    "ScienceQA_VAL Overall": 95.3,
    "ScienceQA_TEST Overall": 95.9,
    "MMStar Overall": 60.6,
    "RealWorldQA Overall": 66.9,
    "POPE Overall": 86.6,
    "SEEDBench2_Plus Overall": 67.5,
    "MMT-Bench_VAL Overall": 60.2
  },
  {
    "Model": "LLaVA-v1-7B",
    "Source": "https://arxiv.org/abs/2304.08485",
    "Parameters": "7.2B",
    "Time": "2023/4/17",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 50.4,
    "CCBench Overall": 12.7,
    "MMBench_TEST_EN Overall": 43.8,
    "MMBench_TEST_CN Overall": 38.7,
    "MMBench_TEST_EN_V11 Overall": 41.6,
    "MMBench_TEST_CN_V11 Overall": 35.8,
    "MME Overall": 1075.5,
    "MMVet Overall": 28.3,
    "MMMU_VAL Overall": 34.1,
    "MathVista Overall": 25.2,
    "HallusionBench Overall": 21.6,
    "LLaVABench Overall": 57.2,
    "AI2D Overall": 48.3,
    "ScienceQA_VAL Overall": 61.9,
    "ScienceQA_TEST Overall": 61.8,
    "MMStar Overall": 27.1,
    "RealWorldQA Overall": 45.8,
    "POPE Overall": 75.9,
    "SEEDBench2_Plus Overall": 31.2,
    "MMT-Bench_VAL Overall": 27.7
  },
  {
    "Model": "LLaVA-v1.5-7B",
    "Source": "https://arxiv.org/abs/2310.03744",
    "Parameters": "7.2B",
    "Time": "2023/10/05",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 65.8,
    "CCBench Overall": 27.5,
    "MMBench_TEST_EN Overall": 66.5,
    "MMBench_TEST_CN Overall": 59.0,
    "MMBench_TEST_EN_V11 Overall": 62.3,
    "MMBench_TEST_CN_V11 Overall": 56.0,
    "MME Overall": 1808.4,
    "MMVet Overall": 32.9,
    "MMMU_VAL Overall": 35.7,
    "MathVista Overall": 25.6,
    "HallusionBench Overall": 27.6,
    "LLaVABench Overall": 61.8,
    "AI2D Overall": 55.5,
    "ScienceQA_VAL Overall": 66.6,
    "ScienceQA_TEST Overall": 69.2,
    "MMStar Overall": 33.1,
    "RealWorldQA Overall": 54.8,
    "TextVQA_VAL Overall": 45.5,
    "ChartQA_TEST Overall": 17.8,
    "OCRVQA_TESTCORE Overall": 60.6,
    "POPE Overall": 86.1,
    "SEEDBench2 Overall": 43.4,
    "SEEDBench2_Plus Overall": 41.3,
    "MMT-Bench_VAL Overall": 48.9,
    "BLINK Overall": 39.7
  },
  {
    "Model": "LLaVA-v1.5-13B",
    "Source": "https://arxiv.org/abs/2310.03744",
    "Parameters": "13.4B",
    "Time": "2023/10/05",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 68.2,
    "CCBench Overall": 30.4,
    "MMBench_TEST_EN Overall": 69.2,
    "MMBench_TEST_CN Overall": 65.0,
    "MMBench_TEST_EN_V11 Overall": 65.8,
    "MMBench_TEST_CN_V11 Overall": 62.1,
    "MME Overall": 1780.8,
    "MMVet Overall": 35.6,
    "MMMU_VAL Overall": 37.0,
    "MathVista Overall": 27.7,
    "HallusionBench Overall": 24.5,
    "LLaVABench Overall": 66.1,
    "AI2D Overall": 61.1,
    "ScienceQA_VAL Overall": 69.5,
    "ScienceQA_TEST Overall": 72.6,
    "MMStar Overall": 34.3,
    "RealWorldQA Overall": 55.3,
    "TextVQA_VAL Overall": 48.9,
    "ChartQA_TEST Overall": 18.2,
    "OCRVQA_TESTCORE Overall": 63.4,
    "POPE Overall": 88.4,
    "SEEDBench2_Plus Overall": 44.6,
    "MMT-Bench_VAL Overall": 52.1,
    "BLINK Overall": 40.9
  },
  {
    "Model": "LLaVA-InternLM-7B (QLoRA)",
    "Source": "https://github.com/InternLM/xtuner/blob/main/xtuner/configs/llava/README.md",
    "Parameters": "7.6B",
    "Time": "2023/12/28",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 65.7,
    "CCBench Overall": 37.3,
    "MMBench_TEST_EN Overall": 69.0,
    "MMBench_TEST_CN Overall": 66.7,
    "MMBench_TEST_EN_V11 Overall": 66.0,
    "MMBench_TEST_CN_V11 Overall": 64.1,
    "MME Overall": 1637.1,
    "MMVet Overall": 32.4,
    "MMMU_VAL Overall": 36.9,
    "MathVista Overall": 27.1,
    "HallusionBench Overall": 28.9,
    "LLaVABench Overall": 59.7,
    "AI2D Overall": 58.0,
    "ScienceQA_VAL Overall": 65.3,
    "ScienceQA_TEST Overall": 68.4,
    "MMStar Overall": 35.5,
    "RealWorldQA Overall": 51.2,
    "TextVQA_VAL Overall": 45.8,
    "ChartQA_TEST Overall": 15.8,
    "OCRVQA_TESTCORE Overall": 57.6,
    "POPE Overall": 86.0,
    "SEEDBench2_Plus Overall": 44.7,
    "MMT-Bench_VAL Overall": 48.3,
    "BLINK Overall": 41.1
  },
  {
    "Model": "LLaVA-v1.5-7B (QLoRA)",
    "Source": "https://github.com/InternLM/xtuner/blob/main/xtuner/configs/llava/README.md",
    "Parameters": "7.2B",
    "Time": "2023/12/28",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 66.4,
    "CCBench Overall": 28.4,
    "MMBench_TEST_EN Overall": 67.7,
    "MMBench_TEST_CN Overall": 61.0,
    "MMBench_TEST_EN_V11 Overall": 64.7,
    "MMBench_TEST_CN_V11 Overall": 59.5,
    "MME Overall": 1716.6,
    "MMVet Overall": 32.2,
    "MMMU_VAL Overall": 33.7,
    "MathVista Overall": 25.1,
    "HallusionBench Overall": 25.2,
    "LLaVABench Overall": 57.2,
    "AI2D Overall": 55.9,
    "ScienceQA_VAL Overall": 68.8,
    "ScienceQA_TEST Overall": 68.7,
    "MMStar Overall": 34.6,
    "RealWorldQA Overall": 53.7,
    "TextVQA_VAL Overall": 49.7,
    "ChartQA_TEST Overall": 18.7,
    "OCRVQA_TESTCORE Overall": 61.0,
    "POPE Overall": 87.2,
    "SEEDBench2_Plus Overall": 41.1,
    "MMT-Bench_VAL Overall": 49.1,
    "BLINK Overall": 41.8
  },
  {
    "Model": "LLaVA-v1.5-13B (QLoRA)",
    "Source": "https://github.com/InternLM/xtuner/blob/main/xtuner/configs/llava/README.md",
    "Parameters": "13.4B",
    "Time": "2023/12/28",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 67.9,
    "CCBench Overall": 32.9,
    "MMBench_TEST_EN Overall": 68.8,
    "MMBench_TEST_CN Overall": 64.7,
    "MMBench_TEST_EN_V11 Overall": 67.0,
    "MMBench_TEST_CN_V11 Overall": 62.5,
    "MME Overall": 1766.0,
    "MMVet Overall": 35.9,
    "MMMU_VAL Overall": 35.2,
    "MathVista Overall": 27.6,
    "HallusionBench Overall": 26.2,
    "LLaVABench Overall": 63.6,
    "AI2D Overall": 61.3,
    "ScienceQA_VAL Overall": 68.9,
    "ScienceQA_TEST Overall": 70.3,
    "MMStar Overall": 40.1,
    "RealWorldQA Overall": 52.0,
    "TextVQA_VAL Overall": 50.7,
    "ChartQA_TEST Overall": 19.0,
    "OCRVQA_TESTCORE Overall": 63.4,
    "POPE Overall": 86.9,
    "SEEDBench2_Plus Overall": 43.6,
    "MMT-Bench_VAL Overall": 50.8,
    "BLINK Overall": 41.3
  },
  {
    "Model": "LLaVA-LLaMA-3-8B",
    "Source": "https://huggingface.co/xtuner/llava-llama-3-8b-v1_1",
    "Parameters": "8B",
    "Time": "2024/05/07",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 70.1,
    "CCBench Overall": 27.8,
    "MMBench_TEST_EN Overall": 71.7,
    "MMBench_TEST_CN Overall": 63.2,
    "MMBench_TEST_EN_V11 Overall": 68.5,
    "MMBench_TEST_CN_V11 Overall": 60.2,
    "MME Overall": 1825.5,
    "MMVet Overall": 34.1,
    "MMMU_VAL Overall": 39.2,
    "MathVista Overall": 40.0,
    "HallusionBench Overall": 28.7,
    "LLaVABench Overall": 69.2,
    "AI2D Overall": 69.9,
    "ScienceQA_VAL Overall": 72.1,
    "ScienceQA_TEST Overall": 72.2,
    "MMStar Overall": 46.1,
    "RealWorldQA Overall": 56.7,
    "TextVQA_VAL Overall": 52.0,
    "ChartQA_TEST Overall": 50.4,
    "OCRVQA_TESTCORE Overall": 61.3,
    "POPE Overall": 87.3,
    "SEEDBench2_Plus Overall": 46.9,
    "MMT-Bench_VAL Overall": 49.5,
    "BLINK Overall": 43.5
  },
  {
    "Model": "LLaVA-InternLM2-7B (QLoRA)",
    "Source": "https://github.com/InternLM/xtuner/blob/main/xtuner/configs/llava/README.md",
    "Parameters": "8.1B",
    "Time": "2024/01/17",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 71.2,
    "CCBench Overall": 42.5,
    "MMBench_TEST_EN Overall": 73.3,
    "MMBench_TEST_CN Overall": 71.7,
    "MMBench_TEST_EN_V11 Overall": 71.9,
    "MMBench_TEST_CN_V11 Overall": 70.0,
    "MME Overall": 1699.5,
    "MMVet Overall": 35.9,
    "MMMU_VAL Overall": 40.1,
    "MathVista Overall": 26.0,
    "HallusionBench Overall": 26.7,
    "LLaVABench Overall": 56.2,
    "AI2D Overall": 63.6,
    "ScienceQA_VAL Overall": 70.9,
    "ScienceQA_TEST Overall": 73.7,
    "MMStar Overall": 38.3,
    "RealWorldQA Overall": 56.1,
    "TextVQA_VAL Overall": 49.3,
    "ChartQA_TEST Overall": 16.0,
    "OCRVQA_TESTCORE Overall": 58.0,
    "POPE Overall": 87.8,
    "SEEDBench2_Plus Overall": 46.7,
    "MMT-Bench_VAL Overall": 53.2
  },
  {
    "Model": "LLaVA-InternLM2-20B (QLoRA)",
    "Source": "https://github.com/InternLM/xtuner/blob/main/xtuner/configs/llava/README.md",
    "Parameters": "20.2B",
    "Time": "2024/01/17",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 70.2,
    "CCBench Overall": 46.3,
    "MMBench_TEST_EN Overall": 75.1,
    "MMBench_TEST_CN Overall": 73.7,
    "MMBench_TEST_EN_V11 Overall": 72.3,
    "MMBench_TEST_CN_V11 Overall": 70.5,
    "MME Overall": 1867.1,
    "MMVet Overall": 37.2,
    "MMMU_VAL Overall": 39.4,
    "MathVista Overall": 25.3,
    "HallusionBench Overall": 26.4,
    "LLaVABench Overall": 63.7,
    "AI2D Overall": 65.4,
    "ScienceQA_VAL Overall": 72.7,
    "ScienceQA_TEST Overall": 73.7,
    "MMStar Overall": 41.9,
    "RealWorldQA Overall": 56.9,
    "TextVQA_VAL Overall": 50.4,
    "ChartQA_TEST Overall": 17.6,
    "OCRVQA_TESTCORE Overall": 61.2,
    "POPE Overall": 88.1,
    "SEEDBench2_Plus Overall": 47.6,
    "MMT-Bench_VAL Overall": 53.3
  },
  {
    "Model": "LLaVA-Next-Vicuna-7B",
    "Source": "https://llava-vl.github.io/blog/2024-01-30-llava-next/",
    "Parameters": "7.1B",
    "Time": "2024/03/25",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 69.6,
    "CCBench Overall": 24.3,
    "MMBench_TEST_EN Overall": 69.2,
    "MMBench_TEST_CN Overall": 62.3,
    "MMBench_TEST_EN_V11 Overall": 66.5,
    "MMBench_TEST_CN_V11 Overall": 59.4,
    "MME Overall": 1769.1,
    "MMVet Overall": 40.2,
    "MMMU_VAL Overall": 37.6,
    "MathVista Overall": 31.5,
    "HallusionBench Overall": 27.6,
    "LLaVABench Overall": 72.7,
    "AI2D Overall": 67.0,
    "ScienceQA_VAL Overall": 68.5,
    "ScienceQA_TEST Overall": 70.3,
    "MMStar Overall": 37.6,
    "RealWorldQA Overall": 57.8,
    "TextVQA_VAL Overall": 64.4,
    "ChartQA_TEST Overall": 55.4,
    "OCRVQA_TESTCORE Overall": 63.8,
    "POPE Overall": 87.5,
    "SEEDBench2_Plus Overall": 51.6,
    "MMT-Bench_VAL Overall": 50.3,
    "BLINK Overall": 41.6
  },
  {
    "Model": "LLaVA-Next-Mistral-7B",
    "Source": "https://llava-vl.github.io/blog/2024-01-30-llava-next/",
    "Parameters": "7.6B",
    "Time": "2024/03/25",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 72.4,
    "CCBench Overall": 30.0,
    "MMBench_TEST_EN Overall": 69.6,
    "MMBench_TEST_CN Overall": 63.3,
    "MMBench_TEST_EN_V11 Overall": 66.5,
    "MMBench_TEST_CN_V11 Overall": 59.8,
    "MME Overall": 1821.2,
    "MMVet Overall": 42.2,
    "MMMU_VAL Overall": 37.0,
    "MathVista Overall": 34.6,
    "HallusionBench Overall": 29.1,
    "LLaVABench Overall": 67.8,
    "AI2D Overall": 69.0,
    "ScienceQA_VAL Overall": 69.5,
    "ScienceQA_TEST Overall": 73.0,
    "MMStar Overall": 38.4,
    "RealWorldQA Overall": 60.0,
    "TextVQA_VAL Overall": 65.2,
    "ChartQA_TEST Overall": 51.8,
    "OCRVQA_TESTCORE Overall": 61.2,
    "POPE Overall": 87.3,
    "SEEDBench2 Overall": 54.7,
    "SEEDBench2_Plus Overall": 49.3,
    "MMT-Bench_VAL Overall": 50.3,
    "BLINK Overall": 41.7
  },
  {
    "Model": "LLaVA-Next-Vicuna-13B",
    "Source": "https://llava-vl.github.io/blog/2024-01-30-llava-next/",
    "Parameters": "13.4B",
    "Time": "2024/03/25",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 71.4,
    "CCBench Overall": 28.8,
    "MMBench_TEST_EN Overall": 70.0,
    "MMBench_TEST_CN Overall": 68.5,
    "MMBench_TEST_EN_V11 Overall": 67.3,
    "MMBench_TEST_CN_V11 Overall": 65.7,
    "MME Overall": 1745.6,
    "MMVet Overall": 44.9,
    "MMMU_VAL Overall": 37.3,
    "MathVista Overall": 34.1,
    "HallusionBench Overall": 31.8,
    "LLaVABench Overall": 73.9,
    "AI2D Overall": 72.2,
    "ScienceQA_VAL Overall": 71.4,
    "ScienceQA_TEST Overall": 73.7,
    "MMStar Overall": 40.4,
    "RealWorldQA Overall": 57.6,
    "TextVQA_VAL Overall": 66.9,
    "ChartQA_TEST Overall": 61.4,
    "OCRVQA_TESTCORE Overall": 65.3,
    "POPE Overall": 87.8,
    "SEEDBench2 Overall": 55.0,
    "SEEDBench2_Plus Overall": 55.6,
    "MMT-Bench_VAL Overall": 52.1,
    "BLINK Overall": 41.2
  },
  {
    "Model": "LLaVA-Next-Yi-34B",
    "Source": "https://llava-vl.github.io/blog/2024-01-30-llava-next/",
    "Parameters": "34.8B",
    "Time": "2024/03/25",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 75.7,
    "CCBench Overall": 49.2,
    "MMBench_TEST_EN Overall": 81.1,
    "MMBench_TEST_CN Overall": 79.0,
    "MMBench_TEST_EN_V11 Overall": 78.5,
    "MMBench_TEST_CN_V11 Overall": 77.1,
    "MME Overall": 2006.5,
    "MMVet Overall": 50.7,
    "MMMU_VAL Overall": 48.8,
    "MathVista Overall": 40.4,
    "HallusionBench Overall": 34.8,
    "LLaVABench Overall": 81.8,
    "AI2D Overall": 78.9,
    "ScienceQA_VAL Overall": 78.8,
    "ScienceQA_TEST Overall": 82.0,
    "MMStar Overall": 51.6,
    "RealWorldQA Overall": 66.0,
    "TextVQA_VAL Overall": 69.3,
    "ChartQA_TEST Overall": 67.6,
    "OCRVQA_TESTCORE Overall": 66.3,
    "POPE Overall": 89.6,
    "SEEDBench2 Overall": 59.0,
    "SEEDBench2_Plus Overall": 65.9,
    "MMT-Bench_VAL Overall": 58.6
  },
  {
    "Model": "LLaVA-Next-Llama3",
    "Source": "https://llava-vl.github.io/blog/2024-01-30-llava-next/",
    "Parameters": "8B",
    "Time": "2024/07/29",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 70.2,
    "CCBench Overall": 29.6,
    "MMBench_TEST_EN Overall": 71.2,
    "MMBench_TEST_CN Overall": 67.3,
    "MMBench_TEST_EN_V11 Overall": 68.4,
    "MMBench_TEST_CN_V11 Overall": 63.7,
    "MME Overall": 1761.8,
    "MMVet Overall": 39.7,
    "MMMU_VAL Overall": 38.4,
    "MathVista Overall": 33.6,
    "HallusionBench Overall": 31.8,
    "LLaVABench Overall": 63.5,
    "AI2D Overall": 68.7,
    "ScienceQA_VAL Overall": 72.4,
    "ScienceQA_TEST Overall": 73.9,
    "MMStar Overall": 42.5,
    "RealWorldQA Overall": 54.6,
    "TextVQA_VAL Overall": 52.8,
    "ChartQA_TEST Overall": 42.7,
    "OCRVQA_TESTCORE Overall": 48.1,
    "POPE Overall": 79.3,
    "SEEDBench2_Plus Overall": 49.9,
    "MMT-Bench_VAL Overall": 49.5,
    "BLINK Overall": 39.9
  },
  {
    "Model": "LLaVA-Next-Interleave-7B-DPO",
    "Source": "https://llava-vl.github.io/blog/2024-01-30-llava-next/",
    "Parameters": "8B",
    "Time": "2024/07/29",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 72.5,
    "CCBench Overall": 43.3,
    "MMBench_TEST_EN Overall": 75.9,
    "MMBench_TEST_CN Overall": 74.6,
    "MMBench_TEST_EN_V11 Overall": 72.7,
    "MMBench_TEST_CN_V11 Overall": 72.0,
    "MME Overall": 1703.1,
    "MMVet Overall": 41.2,
    "MMMU_VAL Overall": 41.8,
    "MathVista Overall": 35.5,
    "HallusionBench Overall": 39.2,
    "LLaVABench Overall": 74.6,
    "AI2D Overall": 74.0,
    "ScienceQA_VAL Overall": 69.5,
    "ScienceQA_TEST Overall": 72.7,
    "MMStar Overall": 44.1,
    "RealWorldQA Overall": 59.3,
    "TextVQA_VAL Overall": 56.7,
    "ChartQA_TEST Overall": 49.1,
    "OCRVQA_TESTCORE Overall": 57.6,
    "POPE Overall": 84.2,
    "SEEDBench2_Plus Overall": 49.6,
    "MMT-Bench_VAL Overall": 54.0,
    "BLINK Overall": 52.5
  },
  {
    "Model": "360VL-70B",
    "Source": "https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5",
    "Parameters": "70B",
    "Time": "2024/05/25",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 73.1,
    "CCBench Overall": 44.9,
    "MMBench_TEST_EN Overall": 78.8,
    "MMBench_TEST_CN Overall": 77.2,
    "MMBench_TEST_EN_V11 Overall": 76.0,
    "MMBench_TEST_CN_V11 Overall": 73.9,
    "MME Overall": 2009.7,
    "MMVet Overall": 24.7,
    "MMMU_VAL Overall": 53.4,
    "MathVista Overall": 38.0,
    "HallusionBench Overall": 34.8,
    "LLaVABench Overall": 75.1,
    "AI2D Overall": 71.9,
    "ScienceQA_VAL Overall": 88.9,
    "ScienceQA_TEST Overall": 87.4,
    "MMStar Overall": 48.1,
    "RealWorldQA Overall": 62.4,
    "POPE Overall": 87.3,
    "SEEDBench2_Plus Overall": 52.3,
    "MMT-Bench_VAL Overall": 58.0,
    "BLINK Overall": 46.1
  },
  {
    "Model": "mPLUG-Owl2",
    "Source": "https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2",
    "Parameters": "8.2B",
    "Time": "2023/11/10",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 64.5,
    "CCBench Overall": 31.0,
    "MMBench_TEST_EN Overall": 66.0,
    "MMBench_TEST_CN Overall": 60.3,
    "MMBench_TEST_EN_V11 Overall": 63.5,
    "MMBench_TEST_CN_V11 Overall": 58.0,
    "MME Overall": 1786.4,
    "MMVet Overall": 35.7,
    "MMMU_VAL Overall": 34.7,
    "MathVista Overall": 25.4,
    "HallusionBench Overall": 29.4,
    "LLaVABench Overall": 59.9,
    "AI2D Overall": 55.7,
    "ScienceQA_VAL Overall": 69.5,
    "ScienceQA_TEST Overall": 69.5,
    "MMStar Overall": 34.8,
    "RealWorldQA Overall": 50.8,
    "TextVQA_VAL Overall": 56.4,
    "ChartQA_TEST Overall": 22.8,
    "OCRVQA_TESTCORE Overall": 65.2,
    "POPE Overall": 84.6,
    "SEEDBench2_Plus Overall": 44.4,
    "MMT-Bench_VAL Overall": 50.4
  },
  {
    "Model": "WeMM",
    "Source": "https://github.com/scenarios/WeMM/tree/main",
    "Parameters": "7B",
    "Time": "2024/06/09",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 75.9,
    "CCBench Overall": 48.0,
    "MMBench_TEST_EN Overall": 79.3,
    "MMBench_TEST_CN Overall": 76.8,
    "MMBench_TEST_EN_V11 Overall": 77.8,
    "MMBench_TEST_CN_V11 Overall": 73.6,
    "MME Overall": 2150.1,
    "MMVet Overall": 45.0,
    "MMMU_VAL Overall": 45.3,
    "MathVista Overall": 54.9,
    "HallusionBench Overall": 47.5,
    "LLaVABench Overall": 70.9,
    "AI2D Overall": 77.9,
    "ScienceQA_VAL Overall": 80.5,
    "ScienceQA_TEST Overall": 83.3,
    "MMStar Overall": 57.0,
    "RealWorldQA Overall": 68.1,
    "POPE Overall": 86.7,
    "SEEDBench2_Plus Overall": 65.0,
    "MMT-Bench_VAL Overall": 57.5
  },
  {
    "Model": "Phi-3-Vision",
    "Source": "https://huggingface.co/microsoft/Phi-3-vision-128k-instruct",
    "Parameters": "4.2B",
    "Time": "2024/05/31",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 70.9,
    "CCBench Overall": 24.1,
    "MMBench_TEST_EN Overall": 73.6,
    "MMBench_TEST_CN Overall": 62.4,
    "MMBench_TEST_EN_V11 Overall": 70.5,
    "MMBench_TEST_CN_V11 Overall": 59.8,
    "MME Overall": 1508.0,
    "MMVet Overall": 44.1,
    "MMMU_VAL Overall": 46.1,
    "MathVista Overall": 44.6,
    "HallusionBench Overall": 39.0,
    "LLaVABench Overall": 63.9,
    "AI2D Overall": 78.4,
    "ScienceQA_VAL Overall": 90.2,
    "ScienceQA_TEST Overall": 90.0,
    "MMStar Overall": 47.7,
    "RealWorldQA Overall": 58.8,
    "TextVQA_VAL Overall": 72.4,
    "ChartQA_TEST Overall": 81.8,
    "OCRVQA_TESTCORE Overall": 61.9,
    "POPE Overall": 83.7,
    "SEEDBench2 Overall": 55.7,
    "SEEDBench2_Plus Overall": 64.2,
    "MMT-Bench_VAL Overall": 54.2,
    "BLINK Overall": 43.3
  },
  {
    "Model": "MMAlaya",
    "Source": "https://huggingface.co/DataCanvas/MMAlaya",
    "Parameters": "7.8B",
    "Time": "2024/01/29",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 57.5,
    "CCBench Overall": 28.6,
    "MMBench_TEST_EN Overall": 58.7,
    "MMBench_TEST_CN Overall": 58.6,
    "MMBench_TEST_EN_V11 Overall": 55.7,
    "MMBench_TEST_CN_V11 Overall": 56.6,
    "MME Overall": 1356.0,
    "MMVet Overall": 22.2,
    "MMMU_VAL Overall": 32.0,
    "MathVista Overall": 22.7,
    "HallusionBench Overall": 32.9,
    "LLaVABench Overall": 50.2,
    "AI2D Overall": 42.3,
    "ScienceQA_VAL Overall": 57.7,
    "ScienceQA_TEST Overall": 60.9,
    "MMStar Overall": 34.3,
    "RealWorldQA Overall": 45.2,
    "TextVQA_VAL Overall": 27.1,
    "ChartQA_TEST Overall": 10.2,
    "OCRVQA_TESTCORE Overall": 51.0,
    "POPE Overall": 82.7,
    "SEEDBench2_Plus Overall": 32.9
  },
  {
    "Model": "MiniCPM-V",
    "Source": "https://huggingface.co/openbmb/MiniCPM-V",
    "Parameters": "3B",
    "Time": "2024/02/07",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 65.6,
    "CCBench Overall": 41.4,
    "MMBench_TEST_EN Overall": 64.1,
    "MMBench_TEST_CN Overall": 62.6,
    "MMBench_TEST_EN_V11 Overall": 61.4,
    "MMBench_TEST_CN_V11 Overall": 59.6,
    "MME Overall": 1650.2,
    "MMVet Overall": 31.1,
    "MMMU_VAL Overall": 38.3,
    "MathVista Overall": 30.6,
    "HallusionBench Overall": 36.2,
    "LLaVABench Overall": 51.3,
    "AI2D Overall": 56.3,
    "ScienceQA_VAL Overall": 74.4,
    "ScienceQA_TEST Overall": 77.0,
    "MMStar Overall": 38.6,
    "RealWorldQA Overall": 51.2,
    "TextVQA_VAL Overall": 56.6,
    "ChartQA_TEST Overall": 44.2,
    "OCRVQA_TESTCORE Overall": 56.4,
    "POPE Overall": 79.5,
    "SEEDBench2_Plus Overall": 43.2,
    "MMT-Bench_VAL Overall": 50.0,
    "BLINK Overall": 40.6
  },
  {
    "Model": "MiniCPM-V-2",
    "Source": "https://huggingface.co/openbmb/MiniCPM-V-2",
    "Parameters": "2.8B",
    "Time": "2024/04/15",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 67.1,
    "CCBench Overall": 45.3,
    "MMBench_TEST_EN Overall": 69.1,
    "MMBench_TEST_CN Overall": 66.5,
    "MMBench_TEST_EN_V11 Overall": 67.0,
    "MMBench_TEST_CN_V11 Overall": 64.5,
    "MME Overall": 1808.2,
    "MMVet Overall": 41.0,
    "MMMU_VAL Overall": 38.2,
    "MathVista Overall": 39.8,
    "HallusionBench Overall": 36.1,
    "LLaVABench Overall": 69.2,
    "AI2D Overall": 62.9,
    "ScienceQA_VAL Overall": 76.3,
    "ScienceQA_TEST Overall": 80.7,
    "MMStar Overall": 39.1,
    "RealWorldQA Overall": 55.8,
    "TextVQA_VAL Overall": 73.2,
    "ChartQA_TEST Overall": 55.6,
    "OCRVQA_TESTCORE Overall": 55.8,
    "POPE Overall": 86.3,
    "SEEDBench2_Plus Overall": 51.9,
    "MMT-Bench_VAL Overall": 53.5,
    "BLINK Overall": 41.2
  },
  {
    "Model": "Ovis1.5-Llama3-8B",
    "Source": "https://huggingface.co/AIDC-AI/Ovis1.5-Llama3-8B",
    "Parameters": "8B",
    "Time": "2024/07/29",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 75.4,
    "CCBench Overall": 36.7,
    "MMBench_TEST_EN Overall": 80.8,
    "MMBench_TEST_CN Overall": 77.6,
    "MMBench_TEST_EN_V11 Overall": 78.1,
    "MMBench_TEST_CN_V11 Overall": 75.1,
    "MME Overall": 1948.5,
    "MMVet Overall": 50.9,
    "MMMU_VAL Overall": 48.3,
    "MathVista Overall": 63.0,
    "HallusionBench Overall": 45.0,
    "LLaVABench Overall": 79.9,
    "AI2D Overall": 82.5,
    "ScienceQA_VAL Overall": 89.3,
    "ScienceQA_TEST Overall": 88.8,
    "MMStar Overall": 57.3,
    "RealWorldQA Overall": 64.2,
    "TextVQA_VAL Overall": 74.0,
    "ChartQA_TEST Overall": 76.4,
    "OCRVQA_TESTCORE Overall": 71.2,
    "POPE Overall": 88.5,
    "SEEDBench2_Plus Overall": 66.4,
    "MMT-Bench_VAL Overall": 60.7,
    "BLINK Overall": 39.8
  },
  {
    "Model": "RekaFlash",
    "Source": "https://www.reka.ai",
    "Parameters": "",
    "Time": "2024/05/07",
    "OpenSource": "No",
    "CCBench Overall": 33.7,
    "MMBench_TEST_EN Overall": 76.8,
    "MMBench_TEST_CN Overall": 74.1,
    "MMBench_TEST_EN_V11 Overall": 74.5,
    "MMBench_TEST_CN_V11 Overall": 71.4,
    "MME Overall": 1861.1,
    "MMVet Overall": 52.5,
    "MMMU_VAL Overall": 44.8,
    "MathVista Overall": 26.6,
    "HallusionBench Overall": 39.2,
    "LLaVABench Overall": 86.3,
    "AI2D Overall": 75.6,
    "ScienceQA_VAL Overall": 85.4,
    "ScienceQA_TEST Overall": 85.5,
    "MMStar Overall": 50.2,
    "RealWorldQA Overall": 55.9,
    "POPE Overall": 80.6,
    "MMT-Bench_VAL Overall": 52.0,
    "BLINK Overall": 31.9
  },
  {
    "Model": "RekaEdge",
    "Source": "https://www.reka.ai",
    "Parameters": "",
    "Time": "2024/05/31",
    "OpenSource": "No",
    "SEEDBench_IMG Overall": 68.4,
    "CCBench Overall": 24.7,
    "MMBench_TEST_EN Overall": 66.4,
    "MMBench_TEST_CN Overall": 59.9,
    "MMBench_TEST_EN_V11 Overall": 63.6,
    "MMBench_TEST_CN_V11 Overall": 56.6,
    "MME Overall": 1641.6,
    "MMVet Overall": 40.5,
    "MMMU_VAL Overall": 42.4,
    "MathVista Overall": 37.6,
    "HallusionBench Overall": 35.1,
    "LLaVABench Overall": 81.6,
    "AI2D Overall": 67.4,
    "ScienceQA_VAL Overall": 71.0,
    "ScienceQA_TEST Overall": 73.5,
    "MMStar Overall": 39.6,
    "RealWorldQA Overall": 57.0,
    "POPE Overall": 83.3,
    "MMT-Bench_VAL Overall": 53.9,
    "BLINK Overall": 41.1
  },
  {
    "Model": "InternVL2-1B",
    "Source": "https://huggingface.co/OpenGVLab/InternVL2-1B",
    "Parameters": "1B",
    "Time": "2024/07/11",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 65.2,
    "CCBench Overall": 76.3,
    "MMBench_TEST_EN Overall": 65.2,
    "MMBench_TEST_CN Overall": 61.0,
    "MMBench_TEST_EN_V11 Overall": 61.6,
    "MMBench_TEST_CN_V11 Overall": 57.7,
    "MME Overall": 1808.9,
    "MMVet Overall": 31.5,
    "MMMU_VAL Overall": 36.7,
    "MathVista Overall": 39.4,
    "HallusionBench Overall": 34.3,
    "LLaVABench Overall": 52.3,
    "AI2D Overall": 63.8,
    "ScienceQA_VAL Overall": 86.8,
    "ScienceQA_TEST Overall": 87.9,
    "MMStar Overall": 45.6,
    "RealWorldQA Overall": 50.2,
    "TextVQA_VAL Overall": 70.9,
    "ChartQA_TEST Overall": 67.8,
    "POPE Overall": 84.9,
    "SEEDBench2_Plus Overall": 54.5,
    "MMT-Bench_VAL Overall": 48.2,
    "BLINK Overall": 38.3
  },
  {
    "Model": "InternVL2-2B",
    "Source": "https://huggingface.co/OpenGVLab/InternVL2-2B",
    "Parameters": "2B",
    "Time": "2024/07/11",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 70.9,
    "CCBench Overall": 74.7,
    "MMBench_TEST_EN Overall": 73.4,
    "MMBench_TEST_CN Overall": 71.2,
    "MMBench_TEST_EN_V11 Overall": 70.2,
    "MMBench_TEST_CN_V11 Overall": 68.9,
    "MME Overall": 1864.3,
    "MMVet Overall": 39.7,
    "MMMU_VAL Overall": 36.3,
    "MathVista Overall": 46.0,
    "HallusionBench Overall": 38.0,
    "LLaVABench Overall": 60.0,
    "AI2D Overall": 74.1,
    "ScienceQA_VAL Overall": 94.3,
    "ScienceQA_TEST Overall": 94.1,
    "MMStar Overall": 49.8,
    "RealWorldQA Overall": 57.4,
    "TextVQA_VAL Overall": 73.4,
    "ChartQA_TEST Overall": 71.7,
    "POPE Overall": 85.2,
    "SEEDBench2_Plus Overall": 59.9,
    "MMT-Bench_VAL Overall": 50.8,
    "BLINK Overall": 42.8
  },
  {
    "Model": "InternVL2-4B",
    "Source": "https://huggingface.co/OpenGVLab/InternVL2-4B",
    "Parameters": "4B",
    "Time": "2024/07/11",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 73.2,
    "CCBench Overall": 66.3,
    "MMBench_TEST_EN Overall": 78.5,
    "MMBench_TEST_CN Overall": 73.9,
    "MMBench_TEST_EN_V11 Overall": 75.8,
    "MMBench_TEST_CN_V11 Overall": 71.4,
    "MME Overall": 2064.6,
    "MMVet Overall": 50.9,
    "MMMU_VAL Overall": 48.3,
    "MathVista Overall": 58.1,
    "HallusionBench Overall": 42.4,
    "LLaVABench Overall": 68.6,
    "AI2D Overall": 79.0,
    "ScienceQA_VAL Overall": 96.0,
    "ScienceQA_TEST Overall": 96.3,
    "MMStar Overall": 53.9,
    "RealWorldQA Overall": 60.5,
    "TextVQA_VAL Overall": 74.7,
    "ChartQA_TEST Overall": 80.7,
    "POPE Overall": 84.6,
    "SEEDBench2_Plus Overall": 63.7,
    "MMT-Bench_VAL Overall": 55.9,
    "BLINK Overall": 45.8
  },
  {
    "Model": "InternVL2-8B",
    "Source": "https://huggingface.co/OpenGVLab/InternVL2-8B",
    "Parameters": "8B",
    "Time": "2024/07/11",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 75.4,
    "CCBench Overall": 77.1,
    "MMBench_TEST_EN Overall": 82.0,
    "MMBench_TEST_CN Overall": 80.9,
    "MMBench_TEST_EN_V11 Overall": 79.5,
    "MMBench_TEST_CN_V11 Overall": 79.3,
    "MME Overall": 2215.1,
    "MMVet Overall": 54.3,
    "MMMU_VAL Overall": 51.2,
    "MathVista Overall": 58.3,
    "HallusionBench Overall": 45.0,
    "LLaVABench Overall": 73.3,
    "AI2D Overall": 83.6,
    "ScienceQA_VAL Overall": 97.4,
    "ScienceQA_TEST Overall": 97.1,
    "MMStar Overall": 61.5,
    "RealWorldQA Overall": 64.2,
    "OCRVQA_TESTCORE Overall": 42.6,
    "POPE Overall": 84.2,
    "SEEDBench2_Plus Overall": 67.3,
    "MMT-Bench_VAL Overall": 59.8,
    "BLINK Overall": 50.9
  },
  {
    "Model": "InternVL2-26B",
    "Source": "https://huggingface.co/OpenGVLab/InternVL2-26B",
    "Parameters": "26B",
    "Time": "2024/07/11",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 76.3,
    "CCBench Overall": 73.9,
    "MMBench_TEST_EN Overall": 83.4,
    "MMBench_TEST_CN Overall": 82.1,
    "MMBench_TEST_EN_V11 Overall": 81.5,
    "MMBench_TEST_CN_V11 Overall": 80.9,
    "MME Overall": 2259.8,
    "MMVet Overall": 60.0,
    "MMMU_VAL Overall": 50.7,
    "MathVista Overall": 59.4,
    "HallusionBench Overall": 51.5,
    "LLaVABench Overall": 93.1,
    "AI2D Overall": 84.5,
    "ScienceQA_VAL Overall": 97.9,
    "ScienceQA_TEST Overall": 97.4,
    "MMStar Overall": 61.0,
    "RealWorldQA Overall": 68.1,
    "TextVQA_VAL Overall": 82.5,
    "ChartQA_TEST Overall": 85.1,
    "OCRVQA_TESTCORE Overall": 43.4,
    "POPE Overall": 86.4,
    "SEEDBench2_Plus Overall": 67.6,
    "MMT-Bench_VAL Overall": 60.4,
    "BLINK Overall": 56.9
  },
  {
    "Model": "InternVL2-40B",
    "Source": "https://huggingface.co/OpenGVLab/InternVL2-40B",
    "Parameters": "40B",
    "Time": "2024/07/11",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 77.4,
    "CCBench Overall": 80.2,
    "MMBench_TEST_EN Overall": 86.8,
    "MMBench_TEST_CN Overall": 86.4,
    "MMBench_TEST_EN_V11 Overall": 85.1,
    "MMBench_TEST_CN_V11 Overall": 84.9,
    "MME Overall": 2293.1,
    "MMVet Overall": 61.8,
    "MMMU_VAL Overall": 55.2,
    "MathVista Overall": 64.0,
    "HallusionBench Overall": 56.5,
    "LLaVABench Overall": 101.4,
    "AI2D Overall": 86.8,
    "ScienceQA_VAL Overall": 97.8,
    "ScienceQA_TEST Overall": 98.5,
    "MMStar Overall": 64.7,
    "RealWorldQA Overall": 70.1,
    "TextVQA_VAL Overall": 83.5,
    "ChartQA_TEST Overall": 83.8,
    "OCRVQA_TESTCORE Overall": 42.9,
    "POPE Overall": 81.9,
    "SEEDBench2_Plus Overall": 69.2,
    "MMT-Bench_VAL Overall": 66.1
  },
  {
    "Model": "InternVL2-Llama3-76B",
    "Source": "https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B",
    "Parameters": "76B",
    "Time": "2024/07/19",
    "OpenSource": "Yes",
    "SEEDBench_IMG Overall": 77.6,
    "CCBench Overall": 81.2,
    "MMBench_TEST_EN Overall": 86.5,
    "MMBench_TEST_CN Overall": 86.5,
    "MMBench_TEST_EN_V11 Overall": 85.5,
    "MMBench_TEST_CN_V11 Overall": 85.5,
    "MME Overall": 2397.6,
    "MMVet Overall": 64.4,
    "MMMU_VAL Overall": 58.3,
    "MathVista Overall": 65.6,
    "HallusionBench Overall": 55.4,
    "LLaVABench Overall": 96.3,
    "AI2D Overall": 87.6,
    "ScienceQA_VAL Overall": 98.2,
    "ScienceQA_TEST Overall": 98.8,
    "MMStar Overall": 67.1,
    "RealWorldQA Overall": 72.7,
    "POPE Overall": 87.3,
    "SEEDBench2_Plus Overall": 70.0,
    "MMT-Bench_VAL Overall": 67.6,
    "BLINK Overall": 57.5
  }
]