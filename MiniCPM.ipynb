{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 01.jpg\n",
      "Processed 02.jpg\n",
      "Processed 03.jpg\n",
      "Processed 04.jpg\n",
      "Processed 05.jpg\n",
      "Processed 06.png\n",
      "Processed 07.png\n",
      "Processed 08.png\n",
      "Processed 09.png\n",
      "Processed 10.png\n",
      "Processed 11.png\n",
      "Processed 12.png\n",
      "Processed 13.png\n",
      "Processed 14.jpg\n",
      "Processed 15.png\n",
      "Processed 15 images\n",
      "Total processing time: 55.46 seconds\n",
      "Average time per image: 3.70 seconds\n",
      "Total output tokens: 486\n",
      "Average output tokens per image: 32.40\n",
      "Results saved to C:\\Users\\Patrick\\Documents\\thesis\\Dataset\\Results\\MiniCPM-V2_analysis_results.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Define paths\n",
    "DATASET_PATH = r\"C:\\Users\\Patrick\\Documents\\thesis\\Dataset\\OwnDataSet\"\n",
    "RESULTS_PATH = r\"C:\\Users\\Patrick\\Documents\\thesis\\Dataset\\Results\"\n",
    "\n",
    "def load_image(image_file):\n",
    "    return Image.open(image_file).convert('RGB')\n",
    "\n",
    "def process_images(model, tokenizer, dataset_path):\n",
    "    results = []\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            img_path = os.path.join(dataset_path, filename)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Load image\n",
    "            image = load_image(img_path)\n",
    "            \n",
    "            # Generate description\n",
    "            question = \"Please describe the image shortly Maximum 150 characters.\"\n",
    "            msgs = [{\"role\": \"user\", \"content\": question}]\n",
    "            params = {\n",
    "                \"sampling\": True,\n",
    "                \"top_p\": 0.8,\n",
    "                \"top_k\": 100,\n",
    "                \"temperature\": 0.7,\n",
    "                \"repetition_penalty\": 1.05,\n",
    "                \"max_new_tokens\": 35\n",
    "            }\n",
    "            \n",
    "            response, _, _ = model.chat(image=image, msgs=msgs, context=None, tokenizer=tokenizer, **params)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            processing_time = end_time - start_time\n",
    "            output_tokens = len(tokenizer.encode(response))\n",
    "            \n",
    "            results.append({\n",
    "                \"filename\": filename,\n",
    "                \"processing_time\": processing_time,\n",
    "                \"output_tokens\": output_tokens,\n",
    "                \"alternative_text\": response\n",
    "            })\n",
    "            \n",
    "            print(f\"Processed {filename}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_results(results, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    output_file = os.path.join(output_path, \"MiniCPM-V2_analysis_results.json\")\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "def main():\n",
    "    # Load model and tokenizer\n",
    "    model_path = 'openbmb/MiniCPM-V-2'\n",
    "    model = AutoModel.from_pretrained(model_path, trust_remote_code=True).to(dtype=torch.bfloat16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device=device)\n",
    "    model.eval()\n",
    "\n",
    "    # Process images\n",
    "    results = process_images(model, tokenizer, DATASET_PATH)\n",
    "\n",
    "    # Print summary\n",
    "    total_time = sum(r[\"processing_time\"] for r in results)\n",
    "    total_output_tokens = sum(r[\"output_tokens\"] for r in results)\n",
    "    num_images = len(results)\n",
    "    \n",
    "    print(f\"Processed {num_images} images\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per image: {total_time/num_images:.2f} seconds\")\n",
    "    print(f\"Total output tokens: {total_output_tokens}\")\n",
    "    print(f\"Average output tokens per image: {total_output_tokens/num_images:.2f}\")\n",
    "\n",
    "    # Save results\n",
    "    save_results(results, RESULTS_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
