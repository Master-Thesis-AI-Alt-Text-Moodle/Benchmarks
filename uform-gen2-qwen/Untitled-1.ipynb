{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.29it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoProcessor\n",
    "\n",
    "model = AutoModel.from_pretrained(\"unum-cloud/uform-gen2-qwen-500m\", trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(\"unum-cloud/uform-gen2-qwen-500m\", trust_remote_code=True)\n",
    "\n",
    "prompt = \"Describe the image accurately\"\n",
    "image = Image.open(\"red_panda.jpg\")\n",
    "\n",
    "inputs = processor(text=[prompt], images=[image], return_tensors=\"pt\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "     output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=151645,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id\n",
    "    )\n",
    "prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "decoded_text = processor.batch_decode(output[:, prompt_len:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The image features a cartoon-style illustration of a person's face, colored in a vibrant red. The person's face is adorned with a red hat and a pair of white glasses. The person's eyes are large and round, with black pupils. The mouth is slightly open, and the nose is pointed upwards. The person's hair is short and straight, with a red color. The face is labeled with various body parts, including the head, neck, shoulders, arms, legs, and feet. The labels are clearly visible and easily readable. The image is a simple yet effective representation of a person's face, with each body part clearly labeled and labeled.<|im_end|>\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Describe the image shortly\"\n",
    "image = Image.open(\"0.png\")\n",
    "\n",
    "inputs = processor(text=[prompt], images=[image], return_tensors=\"pt\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "     output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=151645,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id\n",
    "    )\n",
    "prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "decoded_text = processor.batch_decode(output[:, prompt_len:])[0]\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a collage of photos of a plant and a bird']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "def predict_step(image_paths):\n",
    "  images = []\n",
    "  for image_path in image_paths:\n",
    "    i_image = Image.open(image_path)\n",
    "    if i_image.mode != \"RGB\":\n",
    "      i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "    images.append(i_image)\n",
    "\n",
    "  pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "  pixel_values = pixel_values.to(device)\n",
    "\n",
    "  output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "  preds = [pred.strip() for pred in preds]\n",
    "  return preds\n",
    "\n",
    "\n",
    "predict_step(['61.png']) # ['a woman in a hospital bed with a woman in a hospital bed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ef8f0154b7464ab1f90abcbd673e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 22 files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash attention is not available, using eager attention instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "At least one of the model submodule will be offloaded to disk, please pass along an `offload_folder`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m chat_template_config \u001b[39m=\u001b[39m ChatTemplateConfig(\u001b[39m'\u001b[39m\u001b[39minternvl-internlm2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m backend_config \u001b[39m=\u001b[39m TurbomindEngineConfig(session_len\u001b[39m=\u001b[39m\u001b[39m8192\u001b[39m, offload_folder\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./test\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# specify the path to offload folder\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(model, chat_template_config\u001b[39m=\u001b[39;49mchat_template_config, backend_config\u001b[39m=\u001b[39;49mbackend_config)\n\u001b[0;32m      9\u001b[0m response \u001b[39m=\u001b[39m pipe((\u001b[39m'\u001b[39m\u001b[39mdescribe this image\u001b[39m\u001b[39m'\u001b[39m, image))\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(response\u001b[39m.\u001b[39mtext)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\lmdeploy\\api.py:89\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(model_path, model_name, backend_config, chat_template_config, log_level, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     tp \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m backend_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m backend_config\u001b[39m.\u001b[39mtp\n\u001b[1;32m---> 89\u001b[0m \u001b[39mreturn\u001b[39;00m pipeline_class(model_path,\n\u001b[0;32m     90\u001b[0m                       model_name\u001b[39m=\u001b[39mmodel_name,\n\u001b[0;32m     91\u001b[0m                       backend\u001b[39m=\u001b[39mbackend,\n\u001b[0;32m     92\u001b[0m                       backend_config\u001b[39m=\u001b[39mbackend_config,\n\u001b[0;32m     93\u001b[0m                       chat_template_config\u001b[39m=\u001b[39mchat_template_config,\n\u001b[0;32m     94\u001b[0m                       tp\u001b[39m=\u001b[39mtp,\n\u001b[0;32m     95\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\lmdeploy\\serve\\vl_async_engine.py:21\u001b[0m, in \u001b[0;36mVLAsyncEngine.__init__\u001b[1;34m(self, model_path, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m vision_config \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mvision_config\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m backend_config \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mbackend_config\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m---> 21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvl_encoder \u001b[39m=\u001b[39m ImageEncoder(model_path,\n\u001b[0;32m     22\u001b[0m                                vision_config,\n\u001b[0;32m     23\u001b[0m                                backend_config\u001b[39m=\u001b[39;49mbackend_config)\n\u001b[0;32m     24\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(model_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     25\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbase\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\lmdeploy\\vl\\engine.py:85\u001b[0m, in \u001b[0;36mImageEncoder.__init__\u001b[1;34m(self, model_path, vision_config, backend_config)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m     81\u001b[0m              model_path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m     82\u001b[0m              vision_config: VisionConfig \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     83\u001b[0m              backend_config: Optional[Union[TurbomindEngineConfig,\n\u001b[0;32m     84\u001b[0m                                             PytorchEngineConfig]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 85\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m load_vl_model(model_path, backend_config\u001b[39m=\u001b[39;49mbackend_config)\n\u001b[0;32m     86\u001b[0m     \u001b[39mif\u001b[39;00m vision_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m         vision_config \u001b[39m=\u001b[39m VisionConfig()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\lmdeploy\\vl\\model\\builder.py:55\u001b[0m, in \u001b[0;36mload_vl_model\u001b[1;34m(model_path, with_llm, backend_config)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m module\u001b[39m.\u001b[39mmatch(hf_config):\n\u001b[0;32m     54\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmatching vision model: \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m         \u001b[39mreturn\u001b[39;00m module(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmatching vision model: \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m failed\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\lmdeploy\\vl\\model\\base.py:31\u001b[0m, in \u001b[0;36mVisonModel.__init__\u001b[1;34m(self, model_path, with_llm, max_memory, hf_config)\u001b[0m\n\u001b[0;32m     29\u001b[0m     _, hf_config \u001b[39m=\u001b[39m get_model_arch(model_path)\n\u001b[0;32m     30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhf_config \u001b[39m=\u001b[39m hf_config\n\u001b[1;32m---> 31\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_model()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\lmdeploy\\vl\\model\\internvl.py:99\u001b[0m, in \u001b[0;36mInternVLVisionModel.build_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maccelerate\u001b[39;00m \u001b[39mimport\u001b[39;00m load_checkpoint_and_dispatch\n\u001b[0;32m     98\u001b[0m \u001b[39mwith\u001b[39;00m disable_logging():\n\u001b[1;32m---> 99\u001b[0m     load_checkpoint_and_dispatch(\n\u001b[0;32m    100\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m    101\u001b[0m         checkpoint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_path,\n\u001b[0;32m    102\u001b[0m         device_map\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwith_llm \u001b[39melse\u001b[39;49;00m {\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m},\n\u001b[0;32m    103\u001b[0m         max_memory\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_memory,\n\u001b[0;32m    104\u001b[0m         no_split_module_classes\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mInternVisionEncoderLayer\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    105\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mhalf)\n\u001b[0;32m    107\u001b[0m \u001b[39m# We need eval mode to freeze the weights in model, thus,\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m# avoid randomness in inference.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\accelerate\\big_modeling.py:613\u001b[0m, in \u001b[0;36mload_checkpoint_and_dispatch\u001b[1;34m(model, checkpoint, device_map, max_memory, no_split_module_classes, offload_folder, offload_buffers, dtype, offload_state_dict, skip_keys, preload_module_classes, force_hooks, strict)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[39mif\u001b[39;00m offload_state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m device_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisk\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m device_map\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m    612\u001b[0m     offload_state_dict \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 613\u001b[0m load_checkpoint_in_model(\n\u001b[0;32m    614\u001b[0m     model,\n\u001b[0;32m    615\u001b[0m     checkpoint,\n\u001b[0;32m    616\u001b[0m     device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[0;32m    617\u001b[0m     offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[0;32m    618\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    619\u001b[0m     offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[0;32m    620\u001b[0m     offload_buffers\u001b[39m=\u001b[39;49moffload_buffers,\n\u001b[0;32m    621\u001b[0m     strict\u001b[39m=\u001b[39;49mstrict,\n\u001b[0;32m    622\u001b[0m )\n\u001b[0;32m    623\u001b[0m \u001b[39mif\u001b[39;00m device_map \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\accelerate\\utils\\modeling.py:1692\u001b[0m, in \u001b[0;36mload_checkpoint_in_model\u001b[1;34m(model, checkpoint, device_map, offload_folder, dtype, offload_state_dict, offload_buffers, keep_in_fp32_modules, offload_8bit_bnb, strict)\u001b[0m\n\u001b[0;32m   1689\u001b[0m     check_tied_parameters_on_same_device(tied_params, device_map)\n\u001b[0;32m   1691\u001b[0m \u001b[39mif\u001b[39;00m offload_folder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m device_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisk\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m device_map\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m-> 1692\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1693\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAt least one of the model submodule will be offloaded to disk, please pass along an `offload_folder`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1694\u001b[0m     )\n\u001b[0;32m   1695\u001b[0m \u001b[39melif\u001b[39;00m offload_folder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m device_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisk\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m device_map\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m   1696\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(offload_folder, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: At least one of the model submodule will be offloaded to disk, please pass along an `offload_folder`."
     ]
    }
   ],
   "source": [
    "from lmdeploy import pipeline, TurbomindEngineConfig, ChatTemplateConfig\n",
    "from lmdeploy.vl import load_image\n",
    "\n",
    "model = 'OpenGVLab/Mini-InternVL-Chat-2B-V1-5'\n",
    "image = load_image('https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/tests/data/tiger.jpeg')\n",
    "chat_template_config = ChatTemplateConfig('internvl-internlm2')\n",
    "backend_config = TurbomindEngineConfig(session_len=8192, offload_folder='./test')  # specify the path to offload folder\n",
    "pipe = pipeline(model, chat_template_config=chat_template_config, backend_config=backend_config)\n",
    "response = pipe(('describe this image', image))\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|██████████| 13/13 [00:01<00:00, 12.31it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Patrick\\\\.cache\\\\huggingface\\\\hub\\\\models--unum-cloud--uform-gen2-qwen-500m\\\\snapshots\\\\78dc2e4d600def7698d5fb3733bea4e22dd2f3f9/torch_weight.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m      4\u001b[0m \u001b[39m# If you want to use the PyTorch model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model, processor \u001b[39m=\u001b[39m uform\u001b[39m.\u001b[39;49mget_model(\u001b[39m'\u001b[39;49m\u001b[39munum-cloud/uform-gen2-qwen-500m\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m# Just English\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m#model, processor = uform.get_model('unum-cloud/uform-vl-multilingual-v2') # 21 Languages\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[39m# If you want to use the light-weight portable ONNX model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# Available combinations: cpu & fp32, gpu & fp32, gpu & fp16\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# Check out Unum's Hugging Face space for more details: https://huggingface.co/unum-cloud\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m#model, processor = uform.get_model_onnx('unum-cloud/uform-vl-english-small', 'cpu', 'fp32')\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model, processor \u001b[39m=\u001b[39m uform\u001b[39m.\u001b[39mget_model_onnx(\u001b[39m'\u001b[39m\u001b[39munum-cloud/uform-vl-english-large\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfp16\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\uform\\__init__.py:49\u001b[0m, in \u001b[0;36mget_model\u001b[1;34m(model_name, token)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_model\u001b[39m(model_name: \u001b[39mstr\u001b[39m, token: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VLM:\n\u001b[1;32m---> 49\u001b[0m     config_path, state, tokenizer_path \u001b[39m=\u001b[39m get_checkpoint(model_name, token)\n\u001b[0;32m     51\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(config_path) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     52\u001b[0m         model \u001b[39m=\u001b[39m VLM(load(f), tokenizer_path)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\uform\\__init__.py:43\u001b[0m, in \u001b[0;36mget_checkpoint\u001b[1;34m(model_name, token)\u001b[0m\n\u001b[0;32m     41\u001b[0m model_path \u001b[39m=\u001b[39m snapshot_download(repo_id\u001b[39m=\u001b[39mmodel_name, token\u001b[39m=\u001b[39mtoken)\n\u001b[0;32m     42\u001b[0m config_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m/torch_config.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 43\u001b[0m state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_path\u001b[39m}\u001b[39;49;00m\u001b[39m/torch_weight.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m config_path, state, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m/tokenizer.json\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    984\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 986\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    987\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    988\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    989\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    990\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    434\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 435\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    436\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 416\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Patrick\\\\.cache\\\\huggingface\\\\hub\\\\models--unum-cloud--uform-gen2-qwen-500m\\\\snapshots\\\\78dc2e4d600def7698d5fb3733bea4e22dd2f3f9/torch_weight.pt'"
     ]
    }
   ],
   "source": [
    "import uform\n",
    "from PIL import Image\n",
    "\n",
    "# If you want to use the PyTorch model\n",
    "model, processor = uform.get_model('unum-cloud/uform-gen2-qwen-500m') # Just English\n",
    "#model, processor = uform.get_model('unum-cloud/uform-vl-multilingual-v2') # 21 Languages\n",
    "\n",
    "# If you want to use the light-weight portable ONNX model\n",
    "# Available combinations: cpu & fp32, gpu & fp32, gpu & fp16\n",
    "# Check out Unum's Hugging Face space for more details: https://huggingface.co/unum-cloud\n",
    "#model, processor = uform.get_model_onnx('unum-cloud/uform-vl-english-small', 'cpu', 'fp32')\n",
    "model, processor = uform.get_model_onnx('unum-cloud/uform-vl-english-large', 'gpu', 'fp16')\n",
    "\n",
    "text = 'a small red panda in a zoo'\n",
    "image = Image.open('red_panda.jpg')\n",
    "\n",
    "image_data = processor.preprocess_image(image)\n",
    "text_data = processor.preprocess_text(text)\n",
    "\n",
    "image_features, image_embedding = model.encode_image(image_data, return_features=True)\n",
    "text_features, text_embedding = model.encode_text(text_data, return_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmark on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoProcessor, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# Define paths\n",
    "DATASET_PATH = r\"C:\\Users\\Patrick\\Documents\\thesis\\Dataset\\OwnDataSet\"\n",
    "RESULTS_PATH = r\"C:\\Users\\Patrick\\Documents\\thesis\\Dataset\\Results\"\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [151645]\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def extract_assistant_response(full_response):\n",
    "    # Split the response by roles\n",
    "    parts = full_response.split('assistant\\n')\n",
    "    if len(parts) > 1:\n",
    "        # Return only the last part (assistant's response)\n",
    "        return parts[-1].strip()\n",
    "    return full_response  # Return full response if splitting fails\n",
    "\n",
    "def process_images(model, processor, dataset_path, device):\n",
    "    results = []\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            img_path = os.path.join(dataset_path, filename)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Load and preprocess image\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            pixel_values = processor.feature_extractor(image).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Generate description\n",
    "            prompt = \"Describe the image shortly\"\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": f\" <image>{prompt}\"}\n",
    "            ]\n",
    "            model_inputs = processor.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            attention_mask = torch.ones(\n",
    "                1, model_inputs.shape[1] + processor.num_image_latents - 1\n",
    "            ).to(device)\n",
    "            \n",
    "            inputs = {\n",
    "                \"input_ids\": model_inputs,\n",
    "                \"images\": pixel_values,\n",
    "                \"attention_mask\": attention_mask\n",
    "            }\n",
    "            \n",
    "            stop = StopOnTokens()\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=35,\n",
    "                    stopping_criteria=StoppingCriteriaList([stop])\n",
    "                )\n",
    "            \n",
    "            full_response = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response = extract_assistant_response(full_response)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            processing_time = end_time - start_time\n",
    "            input_tokens = inputs['input_ids'].numel()\n",
    "            output_tokens = outputs.numel()\n",
    "            \n",
    "            results.append({\n",
    "                \"filename\": filename,\n",
    "                \"processing_time\": processing_time,\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"output_tokens\": output_tokens,\n",
    "                \"alternative_text\": response\n",
    "            })\n",
    "            \n",
    "            print(f\"Processed {filename}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_results(results, output_path, device):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    output_file = os.path.join(output_path, f\"uform-gen2-qwen-500m_{device}_analysis_results.json\")\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "def run_benchmark(device):\n",
    "    print(f\"Running benchmark on {device}\")\n",
    "    \n",
    "    # Load model and processor\n",
    "    model_path = 'unum-cloud/uform-gen2-qwen-500m'\n",
    "    model = AutoModel.from_pretrained(model_path, trust_remote_code=True).to(device)\n",
    "    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "    # Process images\n",
    "    results = process_images(model, processor, DATASET_PATH, device)\n",
    "\n",
    "    # Print summary\n",
    "    total_time = sum(r[\"processing_time\"] for r in results)\n",
    "    total_input_tokens = sum(r[\"input_tokens\"] for r in results)\n",
    "    total_output_tokens = sum(r[\"output_tokens\"] for r in results)\n",
    "    num_images = len(results)\n",
    "    \n",
    "    print(f\"Processed {num_images} images\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per image: {total_time/num_images:.2f} seconds\")\n",
    "    print(f\"Total input tokens: {total_input_tokens}\")\n",
    "    print(f\"Total output tokens: {total_output_tokens}\")\n",
    "    print(f\"Average input tokens per image: {total_input_tokens/num_images:.2f}\")\n",
    "    print(f\"Average output tokens per image: {total_output_tokens/num_images:.2f}\")\n",
    "\n",
    "    # Save results\n",
    "    save_results(results, RESULTS_PATH, device)\n",
    "\n",
    "def main():\n",
    "    # Run benchmark on GPU\n",
    "    if torch.cuda.is_available():\n",
    "        run_benchmark(\"cuda\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. Skipping GPU benchmark.\")\n",
    "\n",
    "    # Run benchmark on CPU\n",
    "    run_benchmark(\"cpu\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
